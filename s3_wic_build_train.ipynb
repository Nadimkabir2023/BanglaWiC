{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8d2f6f7",
   "metadata": {},
   "source": [
    "\n",
    " **Creating Bangla WSD Dataset from Gold_Annotated dataset**\n",
    "\n",
    " This script sweeps a folder of gold-annotated sentence files (CSV/XLSX),\n",
    " normalizes and validates their columns, maps each (lemma, sense_id) to a\n",
    " human-readable sense text, assigns robust sentence IDs, and writes a single       consolidated CSV ready for Word Sense Disambiguation (WSD) / WiC-style\n",
    " dataset construction.\n",
    " \n",
    "   1) Locate all \"*_labelled_sentences.csv\" / \".xlsx\" files in INPUT_FOLDER.\n",
    "   2) Read each file (CSV preferred, fallback to Excel) with strict column checks.\n",
    "   3) Coerce sense labels to integer `sense_id` when possible.\n",
    "   4) Attach sense glosses using SENSE_MAP (blank if unknown).\n",
    "   5) Concatenate all rows, sort by (lemma, sense_id), add global running\n",
    "      index + stable `sent_id`, and save as UTF-8 with BOM for Bangla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b27be3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 43 files.\n",
      "Saved 4427 rows to C:\\Users\\Student\\Downloads\\projecting_sentences-main\\projecting_sentences-main\\bangla_wsd.csv\n"
     ]
    }
   ],
   "source": [
    "import os                               # Standard library for paths, filenames, and OS utilities\n",
    "import glob                             # Standard library for filename pattern expansion\n",
    "import pandas as pd                     # Pandas for data loading, cleaning, and saving\n",
    "\n",
    "\n",
    "# paths \n",
    "INPUT_FOLDER = r\"C:\\Users\\Student\\Downloads\\projecting_sentences-main\\projecting_sentences-main\\Dataset\\Gold_dataset\"  # Root directory containing gold dataset files\n",
    "OUTPUT_CSV   = r\"C:\\Users\\Student\\Downloads\\projecting_sentences-main\\projecting_sentences-main\\Dataset\\bangla_wsd_dataset.csv\"  # Consolidated output CSV path\n",
    "\n",
    "\n",
    "# Dictionary mapping lemma -> sense_id -> human-readable sense gloss\n",
    "SENSE_MAP = {                           \n",
    "    'অর্থ': {0:'Money, finance, wealth', 1:'Meaning, sense'},                               \n",
    "    'আগুন': {0:'Passion, intensity, anger', 1:'Flame'},                                     \n",
    "    'এঁটে': {0:'To silence, to seal', 1:'To fasten, stick, close'},                          \n",
    "    'কপাল': {0:'Forehead', 1:'Fate, duck,  destiny'},                                      \n",
    "    'কাটা': {0:'To spend (time), to pass (time)', 1:'To cut, chop'},                       \n",
    "    'গভীর': {0:'Deep', 1:'Profound, intense'},                                              \n",
    "    'গুলি': {0:'Being shot/firing a gun', 1:'Bullet', 2:'Shooting/noise'},                   \n",
    "    'গোলা': {0:'Batter, mixture', 1:'Granary, storage bin, a projectile'},                  \n",
    "    'ঘোর': {0:'State of being under the influence', 1:'To wander, roam, travel, visit'},     \n",
    "    'চড়': {0:'High price,  value', 1:'Get on, to ride, to board'},                         \n",
    "    'চাবি': {0:'Key', 1:'Keyring, bunch of keys'},                                        \n",
    "    'চোখ': {0:'Eye', 1:'Sight, Gaze, or Attention'},                                         \n",
    "    'ঝোলা': {0:'To hang or Suspend something', 1:'A bag'},                                   \n",
    "    'ঠান্ডা': {0:'Feeling ill', 1:'Low temperature; Chilly, Wintry'},                        \n",
    "    'ডাক': {0:'Mail or post', 1:'Call to action', 2:'call,summon'},                          \n",
    "    'তাড়া': {0:'To drive away', 1:'Chase a target'},                                       \n",
    "    'দম': {0:'Stamina', 1:'Steamed', 2:'Out of breath'},                                     \n",
    "    'দল': {0:'Team', 1:'Party or Group'},                                                    \n",
    "    'দাঁড়া': {0:'Wait', 1:'Pause for a moment', 2:'To establish', 3:'To stand'},           \n",
    "    'ধন': {0:'Wealth, money, riches', 1:'Treasure, resource'},                              \n",
    "    'ধরা': {0:'To assume', 1:'To catch, capture, get caught'},                               \n",
    "    'পটল': {0:'Vegetable', 1:'To die,to pass away'},                                         \n",
    "    'পদ': {0:'Dish,  recipe', 1:'Word, phrase', 2:'Position'},                             \n",
    "    'পর': {0:'To wear,  put on', 1:'After (in time), following an event, Later'},             \n",
    "    'পাকা': {0:'Experienced', 1:'Ripe (fruit, crop)'},                                      \n",
    "    'পাড়া': {0:'To lay', 1:'To pick, pluck', 2:'Locality'},                                \n",
    "    'পালা': {0:'A play, Act, performance', 1:'To escape run away, or Flee'},                \n",
    "    'পিচ': {0:'Playing field', 1:'Road surface', 2:'Drug'},                                 \n",
    "    'ফল': {0:'Fruit', 1:'Result, outcome'},                                                \n",
    "    'বর্ণ': {0:'Colour', 1:'Letter, character', 2:'Caste, race'},                      \n",
    "    'বল': {0:'Ball', 1:'To say, speak, state'},                                            \n",
    "    'বিন্দু': {0:'A point or position (in space, geometry)', 1:'A dot, spot, drop, small particle'},  \n",
    "    'মধু': {0:'Honey', 1:'Sweetness, pleasantness, delight'},                                \n",
    "    'মাটি': {0:'Country, Land', 1:'Soil, earth, ground'},                                    \n",
    "    'মাথা': {0:'End', 1:'In mind', 2:'Head'},                                                \n",
    "    'মুখ': {0:'Confronting', 1:'Face', 2:'Speaking'},                                      \n",
    "    'রূপ': {0:'Beauty, appearance', 1:'Role, capacity, function, status'},                   \n",
    "    'সারা': {0:'To cure, heal', 1:'Whole, entire, all over'},                               \n",
    "    'হাত': {0:'Involvement, possession, skill', 1:'Hand'},                                 \n",
    "    'তুলা': {0:'To take (a photo), to capture', 1:'Cotton (processed, used as material', 2:'Cotton (as an agricultural crop or fiber'},  \n",
    "    'পড়া': {0:'To read, study', 1:'To fall, collapse'},                                   \n",
    "    'পথ': {0:'Way, method, or approach', 1:'Road, route, or physical path'},               \n",
    "    'বাড়ি': {0:'House or home', 1:'Household, family, or members of the household'},      \n",
    "}\n",
    "\n",
    "\n",
    "# Function: robust_read\n",
    "# Purpose : Load a labelled sentences file from disk, accepting either CSV or Excel,\n",
    "# enforce column names/availability, and return a standardized DataFrame with the\n",
    "# columns: ['lemma', 'sentence', 'sense', 'start', 'end'].\n",
    "# Inputs  :\n",
    "#   path (str): Relative file path to a *_labelled_sentences.csv/.xlsx file.\n",
    "# Outputs :\n",
    "#   pandas.DataFrame: A dataframe containing exactly the columns\n",
    "#   ['lemma', 'sentence', 'sense', 'start', 'end'] with normalized types.\n",
    "def robust_read(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Read csv if possible; else excel. Ensure needed columns exist.\"\"\"  # Docstring summarizing behavior\n",
    "    try:\n",
    "        df = pd.read_csv(path, encoding='utf-8')            # Try fast CSV read with UTF-8\n",
    "    except Exception:\n",
    "        df = pd.read_excel(path)                            # Fallback to Excel if CSV reading fails\n",
    "    df.columns = [c.strip().lower() for c in df.columns]    # Normalize column names: strip spaces, lowercase\n",
    "    rename_map = {                                          # Map possible source column names to target names\n",
    "        'lemma': 'lemma',                                   # Keep 'lemma' as 'lemma'\n",
    "        'sentence': 'sentence',                             # Keep 'sentence' as 'sentence'\n",
    "        'sense': 'sense',                                   # If already 'sense', keep it\n",
    "        'sense_id': 'sense',                                 # If source uses 'sense_id', normalize to 'sense'\n",
    "        'start': 'start',                                   # Keep 'start'\n",
    "        'end': 'end'                                        # Keep 'end'\n",
    "    }\n",
    "    df = df.rename(columns=rename_map)                      # Apply renaming map\n",
    "    missing = [c for c in ['lemma', 'sentence', 'sense', 'start', 'end'] if c not in df.columns]  # Detect missing required cols\n",
    "    if missing:                                             # If any required columns are missing\n",
    "        raise ValueError(f\"{os.path.basename(path)} is missing columns: {missing}\")  # Raise a clear error\n",
    "    return df[['lemma', 'sentence', 'sense', 'start', 'end']]  # Return standardized subset of columns\n",
    "\n",
    "\n",
    "# Function: sense_text\n",
    "# Purpose : Given a lemma and a raw sense identifier, coerce the identifier to int\n",
    "# and look up a human-readable gloss from SENSE_MAP. If coercion fails or the pair\n",
    "# is unknown, return an empty string (so downstream can keep blanks safely).\n",
    "# Inputs  :\n",
    "#   lemma (str): The lemma key used in SENSE_MAP.\n",
    "#   sense_id: A value convertible to integer sense ID (e.g., '0', 0, '0.0').\n",
    "# Outputs :\n",
    "#   str: The human-readable gloss.\n",
    "def sense_text(lemma: str, sense_id):\n",
    "    try:\n",
    "        i = int(sense_id)                                   # Attempt to coerce sense_id to integer\n",
    "    except Exception:\n",
    "        return ''                                           # On failure, return blank text\n",
    "    return SENSE_MAP.get(lemma, {}).get(i, '')              # Lookup gloss by lemma and integer id (blank if not found)\n",
    "\n",
    "\n",
    "# sweep folder, merge, format\n",
    "all_rows = []                                               # Will collect per-file DataFrames\n",
    "files = sorted(                                             # Build a sorted list of candidate files\n",
    "    glob.glob(os.path.join(INPUT_FOLDER, \"*_labelled_sentences.csv\"))  # Match all CSV labelled files\n",
    "    + glob.glob(os.path.join(INPUT_FOLDER, \"*_labelled_sentences.xlsx\"))  # And all XLSX labelled files\n",
    ")\n",
    "\n",
    "print(f\"Found {len(files)} files.\")                         # Report how many files we found\n",
    "\n",
    "for fp in files:                                            # Iterate over each candidate file path\n",
    "    try:\n",
    "        df = robust_read(fp)                                # Load and standardize the file\n",
    "    except Exception as e:\n",
    "        print(f\"Skip {os.path.basename(fp)} -> {e}\")        # If bad, log and continue\n",
    "        continue\n",
    "\n",
    "    # Ensure sense_id is integer when possible\n",
    "    def to_int_safe(x):                                     # Helper to coerce sense labels to integers robustly\n",
    "        try:\n",
    "            return int(float(x))                            # Accept '0', '0.0', etc.\n",
    "        except Exception:\n",
    "            return None                                     # Return None if not convertible\n",
    "\n",
    "    df['sense_id'] = df['sense'].apply(to_int_safe)         # Create integer `sense_id` column\n",
    "    df = df.dropna(subset=['lemma', 'sentence']).copy()     # Drop rows that lack core fields\n",
    "\n",
    "    # Map senses (text) from the SENSE_MAP; blank if not found\n",
    "    df['senses'] = [sense_text(lem, sid) for lem, sid in zip(df['lemma'], df['sense_id'])]  # Attach gloss text\n",
    "\n",
    "    # Keep requested columns only\n",
    "    out = df[['lemma', 'sentence', 'sense_id', 'start', 'end', 'senses']].copy()  # Final per-file subset\n",
    "    all_rows.append(out)                                      # Stash for later concatenation\n",
    "\n",
    "if not all_rows:                                              # If nothing was read successfully\n",
    "    raise SystemExit(\"No valid files read. Check INPUT_FOLDER and filenames.\")  # Exit early with message\n",
    "\n",
    "\n",
    "\n",
    "# combine, sort within each lemma by sense_id, then assign global running sent_id\n",
    "final_df = pd.concat(all_rows, ignore_index=True)            # Concatenate all per-file DataFrames into one\n",
    "\n",
    "# Sort: for each lemma, sense 0 first, then 1,2,3,...\n",
    "# (NaN sense_id will be placed at the end by default)\n",
    "final_df = final_df.sort_values(by=['lemma', 'sense_id'], kind='mergesort').reset_index(drop=True)  # Stable sort for reproducibility\n",
    "\n",
    "# Global running index that NEVER resets per lemma/file\n",
    "final_df['global_idx'] = range(len(final_df))                # Add a monotonic global index\n",
    "final_df['sent_id'] = final_df.apply(lambda r: f\"bn_{r['sense_id']}_{r['global_idx']}\", axis=1)  # Construct stable sentence IDs\n",
    "\n",
    "# Reorder columns\n",
    "final_df = final_df[['lemma', 'sentence', 'sense_id', 'start', 'end', 'sent_id', 'senses']]  # Column order as required\n",
    "\n",
    "# Save\n",
    "final_df.to_csv(OUTPUT_CSV, index=False, encoding='utf-8-sig')  # Write consolidated CSV with BOM for Bangla compatibility\n",
    "print(f\"Saved {len(final_df)} rows to {OUTPUT_CSV}\")            # Report success with row count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc53618",
   "metadata": {},
   "source": [
    "**Creating Bangla WiC Dataset  — Capped Pairing (MAX_PARTNERS_PER_SENT = 32)**\n",
    "\n",
    "This script transforms a gold-annotated Bangla WSD table into a Word-in-Context (WiC) benchmark. It validates and normalizes the input schema, generates positive (same-sense) and negative (cross-sense) sentence pairs per lemma under a strict per-sentence partner cap, assigns sentences deterministically to train/dev/test using seeded hashing to prevent sentence overlap across splits, balances labels within each split, and writes the results to WiC-formatted JSON files along with a summary report. All randomness is seeded for reproducibility.\n",
    "\n",
    "   1) Read the WSD CSV (lemma, sentence, sense_id, start, end, sent_id[, senses]).\n",
    "   2) Normalize column names and coerce types; keep only rows with valid sense_id.\n",
    "   3) Group rows by lemma to build pairs within each lemma’s examples.\n",
    "   4) Within a lemma, form all unique pairs among sentences sharing the same sense_id.\n",
    "   5) Within a lemma, form pairs across different sense_id values (cross-sense).\n",
    "   6) Canonicalize pair order by sent_id and deduplicate using unordered pair keys.\n",
    "   7) Enforce MAX_PARTNERS_PER_SENT so no sentence exceeds the allowed partner count.\n",
    "   8) Hash each sent_id with the global SEED and map to train/dev/test proportions.\n",
    "   9) Keep only pairs whose two sentences fall in the same split, ensuring no overlap.\n",
    "   10) Re-apply the per-sentence cap independently within each split.\n",
    "   11) Downsample the larger class (if needed) to target ~50/50 positives/negatives per split.\n",
    "   12) Shuffle each split with the seeded RNG.\n",
    "   13) Write wic_train.json, wic_dev.json, wic_test.json (WiC entries include sentences, spans, labels, sense ids, and any sense text).\n",
    "   14) Emit summary.json with totals and class balance per split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221e50c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading WSD\n",
      "Found 43 lemmas.\n",
      "Saved wic_train.json: 5106 pairs\n",
      "Saved wic_dev.json: 218 pairs\n",
      "Saved wic_test.json: 194 pairs\n",
      "Summary: {'train': {'total': 5106, 'pos_neg': (2553, 2553, 0.5)}, 'dev': {'total': 218, 'pos_neg': (109, 109, 0.5)}, 'test': {'total': 194, 'pos_neg': (97, 97, 0.5)}, 'caps': {'MAX_PARTNERS_PER_SENT': 32}}\n"
     ]
    }
   ],
   "source": [
    "import os                                             # Filesystem paths, directory creation\n",
    "import json                                           # JSON serialization for output files\n",
    "import random                                         # Deterministic shuffling and hashing for splits\n",
    "from collections import defaultdict, Counter          # Grouping by sense and tracking partner caps\n",
    "import pandas as pd                                   # Tabular data loading and processing\n",
    "\n",
    "# Configuration\n",
    "WSD_CSV = r'C:\\Users\\Student\\Downloads\\projecting_sentences-main\\projecting_sentences-main\\Dataset\\bangla_wsd_dataset.csv'    # your WSD-style file with: lemma,sentence,sense_id,start,end,sent_id[,senses]\n",
    "OUT_DIR = r'C:\\Users\\Student\\Downloads\\projecting_sentences-main\\projecting_sentences-main\\Dataset\\bangla_wic_dataset_capped'  # Output directory for WiC JSONs and summary\n",
    "os.makedirs(OUT_DIR, exist_ok=True)                  # Create output directory if it does not exist\n",
    "\n",
    "SEED = 42                                            # Global seed for reproducible randomness\n",
    "TRAIN_P, DEV_P, TEST_P = 0.70, 0.15, 0.15            # Proportions for train/dev/test splits (sum to 1)\n",
    "\n",
    "# WiC constraints\n",
    "MAX_PARTNERS_PER_SENT = 32   # cap: each sentence appears with at most 32 partners (across whole dataset)  # Partner cap per sentence across the entire dataset\n",
    "GLOBAL_BALANCE = True        # make each split 50/50 by downsampling the larger class                      # Balance labels within each split\n",
    "\n",
    "rng = random.Random(SEED)                            # Dedicated RNG instance seeded for determinism\n",
    "\n",
    "\n",
    "# Function: load_wsd\n",
    "# Purpose : Load the consolidated WSD CSV, validate required columns, normalize names,\n",
    "# coerce data types, keep only annotated rows, and return a clean DataFrame.\n",
    "# Inputs  :\n",
    "#   path (str): Path to the WSD CSV file.\n",
    "# Returns :\n",
    "#   pandas.DataFrame: Cleaned dataframe with required columns and types.\n",
    "def load_wsd(path):                                   # Define loader for WSD CSV\n",
    "    df = pd.read_csv(path, encoding='utf-8-sig')      # Read CSV (UTF-8 with BOM handling)\n",
    "    cols = {c.lower(): c for c in df.columns}         # Map lowercase column names to originals\n",
    "\n",
    "    required = ['lemma','sentence','sense_id','start','end','sent_id']  # Required schema\n",
    "    missing = [c for c in required if c not in cols]  # Identify missing required columns\n",
    "    if missing:                                       # If any required column is missing\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")  # Fail fast with clear message\n",
    "\n",
    "    df = df.rename(columns={                          # Normalize to canonical column names\n",
    "        cols['lemma']: 'lemma',\n",
    "        cols['sentence']: 'sentence',\n",
    "        cols['sense_id']: 'sense_id',\n",
    "        cols['start']: 'start',\n",
    "        cols['end']: 'end',\n",
    "        cols['sent_id']: 'sent_id'\n",
    "    })\n",
    "    # optional sense text column\n",
    "    if 'senses' in cols:                              # If a senses column exists in source\n",
    "        df['senses'] = df[cols['senses']].astype(str) # Keep it and ensure string type\n",
    "    else:                                             # Otherwise\n",
    "        df['senses'] = ''                             # Initialize a blank senses column\n",
    "\n",
    "    # types + keep only annotated\n",
    "    df['sense_id'] = pd.to_numeric(df['sense_id'], errors='coerce').astype('Int64')  # Coerce to nullable int\n",
    "    df = df[df['sense_id'].notna()].copy()            # Keep rows with a valid sense_id\n",
    "    df['sense_id'] = df['sense_id'].astype(int)       # Cast to concrete int\n",
    "    df['start'] = pd.to_numeric(df['start'], errors='coerce').fillna(0).astype(int)  # Start index as int\n",
    "    df['end']   = pd.to_numeric(df['end'],   errors='coerce').fillna(0).astype(int)  # End index as int\n",
    "    df['sent_id'] = df['sent_id'].astype(str)         # Sentence ID as string\n",
    "    df['lemma']   = df['lemma'].astype(str)           # Lemma as string\n",
    "    df['sentence']= df['sentence'].astype(str)        # Sentence text as string\n",
    "    return df                                         # Return cleaned dataframe\n",
    "\n",
    "\n",
    "# Sentence-based hashing -> split (no sentence overlap)\n",
    "\n",
    "# Function: sentence_bin\n",
    "# Purpose : Assign a sentence deterministically to 'train', 'dev', or 'test' using a\n",
    "# seeded random draw keyed by (SEED|sent_id). Ensures no sentence appears\n",
    "# in more than one split.\n",
    "# Inputs  :\n",
    "#   sent_id (str): Unique sentence identifier.\n",
    "# Returns :\n",
    "#   str: One of {'train', 'dev', 'test'}.\n",
    "def sentence_bin(sent_id: str) -> str:                # Deterministic split assignment\n",
    "    r = random.Random()                               # Fresh RNG instance\n",
    "    r.seed(f\"{SEED}|{sent_id}\")                       # Seed with global seed and sent_id\n",
    "    x = r.random()                                    # Draw uniform value in [0, 1)\n",
    "    if x < TRAIN_P: return 'train'                    # Map into train range\n",
    "    if x < TRAIN_P + DEV_P: return 'dev'              # Map into dev range\n",
    "    return 'test'                                     # Otherwise assign to test\n",
    "\n",
    "\n",
    "# Function: make_pair\n",
    "# Purpose : Create a WiC pair dictionary from two sentence records plus a binary label.\n",
    "# Enforces a canonical order by sent_id to avoid duplicate pair permutations.\n",
    "# Inputs  :\n",
    "#   r1, r2 (row-like): Records with fields used below.\n",
    "#   label (int): 1 if same sense, 0 if different sense.\n",
    "# Returns :\n",
    "#   dict: WiC pair entry with spans, labels, and optional sense texts.\n",
    "def make_pair(r1, r2, label):                         # Construct a single WiC pair\n",
    "    # canonicalize order by sent_id to avoid duplicate pairs\n",
    "    a, b = (r1, r2) if r1['sent_id'] <= r2['sent_id'] else (r2, r1)  # Deterministic ordering\n",
    "    return {                                          # Build pair payload\n",
    "        \"lemma\": a['lemma'],                          # Lemma shared by both sentences\n",
    "        \"sentence1\": a['sentence'],                   # First sentence text\n",
    "        \"sentence2\": b['sentence'],                   # Second sentence text\n",
    "        \"sent_id1\": a['sent_id'],                     # First sentence ID\n",
    "        \"sent_id2\": b['sent_id'],                     # Second sentence ID\n",
    "        \"start1\": int(a['start']),                    # Span start in sentence1\n",
    "        \"end1\": int(a['end']),                        # Span end in sentence1\n",
    "        \"start2\": int(b['start']),                    # Span start in sentence2\n",
    "        \"end2\": int(b['end']),                        # Span end in sentence2\n",
    "        \"label\": int(label),                          # 1 = same sense, 0 = different sense\n",
    "        \"sense_id1\": int(a['sense_id']),              # Sense ID for sentence1\n",
    "        \"sense_id2\": int(b['sense_id']),              # Sense ID for sentence2\n",
    "        \"sense1\": a.get('senses', ''),                # Optional sense text for sentence1\n",
    "        \"sense2\": b.get('senses', '')                 # Optional sense text for sentence2\n",
    "    }\n",
    "\n",
    "\n",
    "# Function: build_pairs_capped\n",
    "# Purpose : For a given lemma subset, generate positive pairs (within-sense) and\n",
    "# negative pairs (cross-sense) while enforcing a per-sentence partner cap and\n",
    "# preventing duplicate unordered pairs.\n",
    "# Inputs  :\n",
    "#   df_lem (DataFrame): Rows for a single lemma.\n",
    "#   max_partners (int or None): Partner cap per sentence; None disables the cap.\n",
    "# Returns :\n",
    "#   (list, list): Lists of positive and negative pair dicts.\n",
    "def build_pairs_capped(df_lem, max_partners=MAX_PARTNERS_PER_SENT):  # Pair generation with cap\n",
    "    # group rows by sense\n",
    "    by_sense = defaultdict(list)                    # Map sense_id → list of rows\n",
    "    for _, r in df_lem.iterrows():                  # Iterate lemma rows\n",
    "        by_sense[int(r['sense_id'])].append(r)      # Bucket row under its sense\n",
    "\n",
    "    senses = sorted(by_sense.keys())                # Sorted list of senses for determinism\n",
    "    for s in senses:                                # For each sense bucket\n",
    "        rng.shuffle(by_sense[s])                    # Shuffle rows to diversify pairings\n",
    "\n",
    "    used_pairs = set()        # track (sent_id1, sent_id2) to avoid duplicates  # Unordered pair keys to deduplicate\n",
    "    partner_count = Counter() # per-sentence partner count                      # Track partners per sentence\n",
    "    pos_pairs, neg_pairs = [], []                   # Accumulators for outputs\n",
    "\n",
    "    unlimited = (max_partners is None)              # Flag indicating no cap\n",
    "\n",
    "    # helper: try adding a pair if both sentences under cap (or unlimited)\n",
    "    def try_add_pair(r1, r2, label):                # Local helper to add a pair under constraints\n",
    "        if r1['sent_id'] == r2['sent_id']:          # Skip pairing a sentence with itself\n",
    "            return                                  # No action\n",
    "        key = tuple(sorted((r1['sent_id'], r2['sent_id'])))  # Unordered key for deduplication\n",
    "        if key in used_pairs:                       # If already paired\n",
    "            return                                  # Skip duplicate\n",
    "        if not unlimited:                           # Enforce partner caps when enabled\n",
    "            if partner_count[r1['sent_id']] >= max_partners: return  # Respect cap for r1\n",
    "            if partner_count[r2['sent_id']] >= max_partners: return  # Respect cap for r2\n",
    "        p = make_pair(r1, r2, label)                # Create WiC pair\n",
    "        used_pairs.add(key)                         # Record the pair key\n",
    "        partner_count[r1['sent_id']] += 1           # Increment partner count for r1\n",
    "        partner_count[r2['sent_id']] += 1           # Increment partner count for r2\n",
    "        (pos_pairs if label == 1 else neg_pairs).append(p)  # Append to positive or negative list\n",
    "\n",
    "    # positives\n",
    "    for s in senses:                                # For each sense bucket\n",
    "        rows = by_sense[s]                          # Rows within the same sense\n",
    "        for i in range(len(rows)):                  # Pairwise iteration (upper triangle)\n",
    "            for j in range(i+1, len(rows)):         # Avoid symmetric duplicates\n",
    "                try_add_pair(rows[i], rows[j], label=1)  # Add positive pair\n",
    "\n",
    "    # negatives (cross-sense)\n",
    "    for s in senses:                                # For each sense bucket\n",
    "        rows_s = by_sense[s]                        # Rows in the current sense\n",
    "        others = [r for t in senses if t != s for r in by_sense[t]]  # All rows in other senses\n",
    "        rng.shuffle(others)                          # Shuffle cross-sense candidates\n",
    "        for r1 in rows_s:                            # For each row in current sense\n",
    "            if not unlimited and partner_count[r1['sent_id']] >= max_partners:  # Cap check for r1\n",
    "                continue                             # Skip if capped\n",
    "            for r2 in others:                        # Iterate cross-sense rows\n",
    "                if not unlimited and partner_count[r1['sent_id']] >= max_partners:  # Re-check during loop\n",
    "                    break                             # Stop if r1 reached cap\n",
    "                try_add_pair(r1, r2, label=0)        # Add negative pair\n",
    "\n",
    "    return pos_pairs, neg_pairs                      # Return both lists\n",
    "\n",
    "\n",
    "# Function: split_and_balance\n",
    "# Purpose : Assign pairs to train/dev/test based on deterministic per-sentence hashing\n",
    "# (both sentences must land in the same split), enforce an optional per-sentence\n",
    "# cap within each split, and balance labels per split if requested.\n",
    "# Inputs  :\n",
    "#   pairs (list): All WiC pair dicts.\n",
    "#   per_sentence_cap (int or None): Cap per sentence inside each split; None disables it.\n",
    "#   global_balance (bool): Whether to balance labels per split.\n",
    "# Returns :\n",
    "#   dict: {'train': [...], 'dev': [...], 'test': [...]} with processed pairs.\n",
    "def split_and_balance(pairs, per_sentence_cap=MAX_PARTNERS_PER_SENT, global_balance=True):  # Split and balance\n",
    "    # keep only pairs whose two sentences hash to the same split\n",
    "    tmp = {'train': [], 'dev': [], 'test': []}       # Containers for intermediate splits\n",
    "    for p in pairs:                                  # Iterate all pairs\n",
    "        b1, b2 = sentence_bin(p['sent_id1']), sentence_bin(p['sent_id2'])  # Compute bins for both sentences\n",
    "        if b1 == b2:                                 # Only keep if both map to the same split\n",
    "            tmp[b1].append(p)                        # Append to that split\n",
    "\n",
    "    # enforce per-split cap (skip if None)\n",
    "    def enforce_split_cap(arr):                      # Local helper to apply per-sentence cap\n",
    "        if per_sentence_cap is None:                 # If cap is disabled\n",
    "            return list(arr)                         # Return all pairs unmodified\n",
    "        counts = Counter()                           # Per-sentence counts inside this split\n",
    "        out = []                                     # Output list after capping\n",
    "        for p in rng.sample(arr, len(arr)):          # Shuffle then greedily keep under caps\n",
    "            a, b = p['sent_id1'], p['sent_id2']      # Sentence IDs\n",
    "            if counts[a] < per_sentence_cap and counts[b] < per_sentence_cap:  # Check both caps\n",
    "                out.append(p)                        # Keep the pair\n",
    "                counts[a] += 1                       # Increment for a\n",
    "                counts[b] += 1                       # Increment for b\n",
    "        return out                                   # Return capped list\n",
    "\n",
    "    for split in tmp:                                # For each split key\n",
    "        tmp[split] = enforce_split_cap(tmp[split])   # Apply per-sentence cap\n",
    "\n",
    "    if global_balance:                               # If balancing is enabled\n",
    "        balanced = {}                                # Output dict for balanced splits\n",
    "        for split in ['train', 'dev', 'test']:       # Process splits in fixed order\n",
    "            arr = tmp[split]                         # Pairs in the current split\n",
    "            pos = [p for p in arr if p['label'] == 1]  # Positive pairs\n",
    "            neg = [p for p in arr if p['label'] == 0]  # Negative pairs\n",
    "            rng.shuffle(pos); rng.shuffle(neg)       # Shuffle class lists\n",
    "            m = min(len(pos), len(neg))              # Target balanced size\n",
    "            balanced[split] = pos[:m] + neg[:m]      # Truncate to balance\n",
    "            rng.shuffle(balanced[split])             # Shuffle final split\n",
    "        return balanced                              # Return balanced splits\n",
    "\n",
    "    return tmp                                       # Return unbalanced splits if balancing disabled\n",
    "\n",
    "\n",
    "# Function: main\n",
    "# Purpose : Orchestrate the full pipeline: load WSD rows, generate capped WiC pairs,\n",
    "# split and balance them, write train/dev/test JSON files, and save summary.\n",
    "# Inputs  :\n",
    "#   uses constants from the config section.\n",
    "# Returns :\n",
    "#   writes files to OUT_DIR and prints a summary.\n",
    "def main():                                          # Program entry point\n",
    "    print(\"Loading WSD\")                             # Status message\n",
    "    df = load_wsd(WSD_CSV)                           # Load and clean WSD dataframe\n",
    "\n",
    "    all_pairs = []                                   # Accumulator for all pairs across lemmas\n",
    "    lemmas = sorted(df['lemma'].unique())            # Unique lemmas in deterministic order\n",
    "    print(f\"Found {len(lemmas)} lemmas.\")            # Report number of lemmas discovered\n",
    "\n",
    "    # build pairs per lemma with per-sentence cap\n",
    "    for lem in lemmas:                               # Iterate each lemma\n",
    "        sub = df[df['lemma'] == lem].copy()          # Subset rows for this lemma\n",
    "        if sub.shape[0] < 2:                         # Skip lemmas with fewer than 2 examples\n",
    "            continue                                 # Proceed to next lemma\n",
    "        pos_pairs, neg_pairs = build_pairs_capped(sub, max_partners=MAX_PARTNERS_PER_SENT)  # Generate pairs\n",
    "        if not pos_pairs and not neg_pairs:          # If no pairs were produced\n",
    "            continue                                 # Proceed to next lemma\n",
    "        pairs = pos_pairs + neg_pairs                # Merge positive and negative pairs\n",
    "        all_pairs.extend(pairs)                      # Accumulate\n",
    "\n",
    "    # split & enforce constraints\n",
    "    splits = split_and_balance(all_pairs, per_sentence_cap=MAX_PARTNERS_PER_SENT, global_balance=GLOBAL_BALANCE)  # Split and balance\n",
    "    train, dev, test = splits['train'], splits['dev'], splits['test']  # Unpack splits\n",
    "\n",
    "    # final shuffle & dump\n",
    "    rng.shuffle(train); rng.shuffle(dev); rng.shuffle(test)  # Shuffle each split before saving\n",
    "\n",
    "    def dump_json(filename, data):                  # Helper to save a split to JSON\n",
    "        path = os.path.join(OUT_DIR, filename)      # Compose output path\n",
    "        with open(path, 'w', encoding='utf-8') as f:  # Open file for writing\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)  # Write JSON with indentation\n",
    "        print(f\"Saved {filename}: {len(data)} pairs\")  # Report save status\n",
    "\n",
    "    dump_json('wic_train.json', train)              # Save training split\n",
    "    dump_json('wic_dev.json',   dev)                # Save development split\n",
    "    dump_json('wic_test.json',  test)               # Save test split\n",
    "\n",
    "    # small summary\n",
    "    def stats(arr):                                  # Helper to compute (pos, neg, pos_ratio)\n",
    "        if not arr: return (0,0,0.0)                 # Handle empty splits\n",
    "        pos = sum(1 for x in arr if x['label']==1)   # Count positives\n",
    "        neg = len(arr) - pos                         # Count negatives\n",
    "        return (pos, neg, round(pos/len(arr), 3))    # Return counts and positive ratio\n",
    "\n",
    "    summary = {                                      # Build summary information\n",
    "        'train': {'total': len(train), 'pos_neg': stats(train)},  # Train totals and balance\n",
    "        'dev':   {'total': len(dev),   'pos_neg': stats(dev)},    # Dev totals and balance\n",
    "        'test':  {'total': len(test),  'pos_neg': stats(test)},   # Test totals and balance\n",
    "        'caps':  {'MAX_PARTNERS_PER_SENT': MAX_PARTNERS_PER_SENT} # Cap configuration\n",
    "    }\n",
    "    with open(os.path.join(OUT_DIR, 'summary.json'), 'w', encoding='utf-8') as f:  # Open summary path\n",
    "        json.dump(summary, f, ensure_ascii=False, indent=2)                         # Write summary JSON\n",
    "    print(\"Summary:\", summary)                    # Print summary to console\n",
    "\n",
    "if __name__ == \"__main__\":                        # Standard Python entry guard\n",
    "    main()                                        # Execute the pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a4ae1b",
   "metadata": {},
   "source": [
    "**Creating Bangla WiC Dataset  — Uncapped Pairing (No Partner Cap, MAX_PARTNERS_PER_SENT = None)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26052774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading WSD\n",
      "Found 43 lemmas.\n",
      "Saved wic_train.json: 103714 pairs\n",
      "Saved wic_dev.json: 4714 pairs\n",
      "Saved wic_test.json: 4270 pairs\n",
      "Summary: {'train': {'total': 103714, 'pos_neg': (51857, 51857, 0.5)}, 'dev': {'total': 4714, 'pos_neg': (2357, 2357, 0.5)}, 'test': {'total': 4270, 'pos_neg': (2135, 2135, 0.5)}, 'caps': {'MAX_PARTNERS_PER_SENT': None}}\n"
     ]
    }
   ],
   "source": [
    "import os                                             # Filesystem paths, directory creation\n",
    "import json                                           # JSON serialization for output files\n",
    "import random                                         # Deterministic shuffling and hashing for splits\n",
    "from collections import defaultdict, Counter          # Grouping by sense and tracking partner caps\n",
    "import pandas as pd                                   # Tabular data loading and processing\n",
    "\n",
    "# Configuration\n",
    "WSD_CSV = r'C:\\Users\\Student\\Downloads\\projecting_sentences-main\\projecting_sentences-main\\Dataset\\bangla_wsd_dataset.csv'    # your WSD-style file with: lemma,sentence,sense_id,start,end,sent_id[,senses]\n",
    "OUT_DIR = r'C:\\Users\\Student\\Downloads\\projecting_sentences-main\\projecting_sentences-main\\Dataset\\bangla_wic_dataset_uncapped'  # Output directory for WiC JSONs and summary\n",
    "os.makedirs(OUT_DIR, exist_ok=True)                  # Create output directory if it does not exist\n",
    "\n",
    "SEED = 42                                            # Global seed for reproducible randomness\n",
    "TRAIN_P, DEV_P, TEST_P = 0.70, 0.15, 0.15            # Proportions for train/dev/test splits (sum to 1)\n",
    "\n",
    "# WiC constraints\n",
    "MAX_PARTNERS_PER_SENT = None   # cap: each sentence appears with at most 32 partners (across whole dataset)  # Partner cap per sentence across the entire dataset\n",
    "GLOBAL_BALANCE = True        # make each split 50/50 by downsampling the larger class                      # Balance labels within each split\n",
    "\n",
    "rng = random.Random(SEED)                            # Dedicated RNG instance seeded for determinism\n",
    "\n",
    "\n",
    "# Function: load_wsd\n",
    "# Purpose : Load the consolidated WSD CSV, validate required columns, normalize names,\n",
    "# coerce data types, keep only annotated rows, and return a clean DataFrame.\n",
    "# Inputs  :\n",
    "#   path (str): Path to the WSD CSV file.\n",
    "# Returns :\n",
    "#   pandas.DataFrame: Cleaned dataframe with required columns and types.\n",
    "def load_wsd(path):                                   # Define loader for WSD CSV\n",
    "    df = pd.read_csv(path, encoding='utf-8-sig')      # Read CSV (UTF-8 with BOM handling)\n",
    "    cols = {c.lower(): c for c in df.columns}         # Map lowercase column names to originals\n",
    "\n",
    "    required = ['lemma','sentence','sense_id','start','end','sent_id']  # Required schema\n",
    "    missing = [c for c in required if c not in cols]  # Identify missing required columns\n",
    "    if missing:                                       # If any required column is missing\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")  # Fail fast with clear message\n",
    "\n",
    "    df = df.rename(columns={                          # Normalize to canonical column names\n",
    "        cols['lemma']: 'lemma',\n",
    "        cols['sentence']: 'sentence',\n",
    "        cols['sense_id']: 'sense_id',\n",
    "        cols['start']: 'start',\n",
    "        cols['end']: 'end',\n",
    "        cols['sent_id']: 'sent_id'\n",
    "    })\n",
    "    # optional sense text column\n",
    "    if 'senses' in cols:                              # If a senses column exists in source\n",
    "        df['senses'] = df[cols['senses']].astype(str) # Keep it and ensure string type\n",
    "    else:                                             # Otherwise\n",
    "        df['senses'] = ''                             # Initialize a blank senses column\n",
    "\n",
    "    # types + keep only annotated\n",
    "    df['sense_id'] = pd.to_numeric(df['sense_id'], errors='coerce').astype('Int64')  # Coerce to nullable int\n",
    "    df = df[df['sense_id'].notna()].copy()            # Keep rows with a valid sense_id\n",
    "    df['sense_id'] = df['sense_id'].astype(int)       # Cast to concrete int\n",
    "    df['start'] = pd.to_numeric(df['start'], errors='coerce').fillna(0).astype(int)  # Start index as int\n",
    "    df['end']   = pd.to_numeric(df['end'],   errors='coerce').fillna(0).astype(int)  # End index as int\n",
    "    df['sent_id'] = df['sent_id'].astype(str)         # Sentence ID as string\n",
    "    df['lemma']   = df['lemma'].astype(str)           # Lemma as string\n",
    "    df['sentence']= df['sentence'].astype(str)        # Sentence text as string\n",
    "    return df                                         # Return cleaned dataframe\n",
    "\n",
    "\n",
    "# Sentence-based hashing -> split (no sentence overlap)\n",
    "\n",
    "# Function: sentence_bin\n",
    "# Purpose : Assign a sentence deterministically to 'train', 'dev', or 'test' using a\n",
    "# seeded random draw keyed by (SEED|sent_id). Ensures no sentence appears\n",
    "# in more than one split.\n",
    "# Inputs  :\n",
    "#   sent_id (str): Unique sentence identifier.\n",
    "# Returns :\n",
    "#   str: One of {'train', 'dev', 'test'}.\n",
    "def sentence_bin(sent_id: str) -> str:                # Deterministic split assignment\n",
    "    r = random.Random()                               # Fresh RNG instance\n",
    "    r.seed(f\"{SEED}|{sent_id}\")                       # Seed with global seed and sent_id\n",
    "    x = r.random()                                    # Draw uniform value in [0, 1)\n",
    "    if x < TRAIN_P: return 'train'                    # Map into train range\n",
    "    if x < TRAIN_P + DEV_P: return 'dev'              # Map into dev range\n",
    "    return 'test'                                     # Otherwise assign to test\n",
    "\n",
    "\n",
    "# Function: make_pair\n",
    "# Purpose : Create a WiC pair dictionary from two sentence records plus a binary label.\n",
    "# Enforces a canonical order by sent_id to avoid duplicate pair permutations.\n",
    "# Inputs  :\n",
    "#   r1, r2 (row-like): Records with fields used below.\n",
    "#   label (int): 1 if same sense, 0 if different sense.\n",
    "# Returns :\n",
    "#   dict: WiC pair entry with spans, labels, and optional sense texts.\n",
    "def make_pair(r1, r2, label):                         # Construct a single WiC pair\n",
    "    # canonicalize order by sent_id to avoid duplicate pairs\n",
    "    a, b = (r1, r2) if r1['sent_id'] <= r2['sent_id'] else (r2, r1)  # Deterministic ordering\n",
    "    return {                                          # Build pair payload\n",
    "        \"lemma\": a['lemma'],                          # Lemma shared by both sentences\n",
    "        \"sentence1\": a['sentence'],                   # First sentence text\n",
    "        \"sentence2\": b['sentence'],                   # Second sentence text\n",
    "        \"sent_id1\": a['sent_id'],                     # First sentence ID\n",
    "        \"sent_id2\": b['sent_id'],                     # Second sentence ID\n",
    "        \"start1\": int(a['start']),                    # Span start in sentence1\n",
    "        \"end1\": int(a['end']),                        # Span end in sentence1\n",
    "        \"start2\": int(b['start']),                    # Span start in sentence2\n",
    "        \"end2\": int(b['end']),                        # Span end in sentence2\n",
    "        \"label\": int(label),                          # 1 = same sense, 0 = different sense\n",
    "        \"sense_id1\": int(a['sense_id']),              # Sense ID for sentence1\n",
    "        \"sense_id2\": int(b['sense_id']),              # Sense ID for sentence2\n",
    "        \"sense1\": a.get('senses', ''),                # Optional sense text for sentence1\n",
    "        \"sense2\": b.get('senses', '')                 # Optional sense text for sentence2\n",
    "    }\n",
    "\n",
    "\n",
    "# Function: build_pairs_capped\n",
    "# Purpose : For a given lemma subset, generate positive pairs (within-sense) and\n",
    "# negative pairs (cross-sense) while enforcing a per-sentence partner cap and\n",
    "# preventing duplicate unordered pairs.\n",
    "# Inputs  :\n",
    "#   df_lem (DataFrame): Rows for a single lemma.\n",
    "#   max_partners (int or None): Partner cap per sentence; None disables the cap.\n",
    "# Returns :\n",
    "#   (list, list): Lists of positive and negative pair dicts.\n",
    "def build_pairs_capped(df_lem, max_partners=MAX_PARTNERS_PER_SENT):  # Pair generation with cap\n",
    "    # group rows by sense\n",
    "    by_sense = defaultdict(list)                    # Map sense_id → list of rows\n",
    "    for _, r in df_lem.iterrows():                  # Iterate lemma rows\n",
    "        by_sense[int(r['sense_id'])].append(r)      # Bucket row under its sense\n",
    "\n",
    "    senses = sorted(by_sense.keys())                # Sorted list of senses for determinism\n",
    "    for s in senses:                                # For each sense bucket\n",
    "        rng.shuffle(by_sense[s])                    # Shuffle rows to diversify pairings\n",
    "\n",
    "    used_pairs = set()        # track (sent_id1, sent_id2) to avoid duplicates  # Unordered pair keys to deduplicate\n",
    "    partner_count = Counter() # per-sentence partner count                      # Track partners per sentence\n",
    "    pos_pairs, neg_pairs = [], []                   # Accumulators for outputs\n",
    "\n",
    "    unlimited = (max_partners is None)              # Flag indicating no cap\n",
    "\n",
    "    # helper: try adding a pair if both sentences under cap (or unlimited)\n",
    "    def try_add_pair(r1, r2, label):                # Local helper to add a pair under constraints\n",
    "        if r1['sent_id'] == r2['sent_id']:          # Skip pairing a sentence with itself\n",
    "            return                                  # No action\n",
    "        key = tuple(sorted((r1['sent_id'], r2['sent_id'])))  # Unordered key for deduplication\n",
    "        if key in used_pairs:                       # If already paired\n",
    "            return                                  # Skip duplicate\n",
    "        if not unlimited:                           # Enforce partner caps when enabled\n",
    "            if partner_count[r1['sent_id']] >= max_partners: return  # Respect cap for r1\n",
    "            if partner_count[r2['sent_id']] >= max_partners: return  # Respect cap for r2\n",
    "        p = make_pair(r1, r2, label)                # Create WiC pair\n",
    "        used_pairs.add(key)                         # Record the pair key\n",
    "        partner_count[r1['sent_id']] += 1           # Increment partner count for r1\n",
    "        partner_count[r2['sent_id']] += 1           # Increment partner count for r2\n",
    "        (pos_pairs if label == 1 else neg_pairs).append(p)  # Append to positive or negative list\n",
    "\n",
    "    # positives\n",
    "    for s in senses:                                # For each sense bucket\n",
    "        rows = by_sense[s]                          # Rows within the same sense\n",
    "        for i in range(len(rows)):                  # Pairwise iteration (upper triangle)\n",
    "            for j in range(i+1, len(rows)):         # Avoid symmetric duplicates\n",
    "                try_add_pair(rows[i], rows[j], label=1)  # Add positive pair\n",
    "\n",
    "    # negatives (cross-sense)\n",
    "    for s in senses:                                # For each sense bucket\n",
    "        rows_s = by_sense[s]                        # Rows in the current sense\n",
    "        others = [r for t in senses if t != s for r in by_sense[t]]  # All rows in other senses\n",
    "        rng.shuffle(others)                          # Shuffle cross-sense candidates\n",
    "        for r1 in rows_s:                            # For each row in current sense\n",
    "            if not unlimited and partner_count[r1['sent_id']] >= max_partners:  # Cap check for r1\n",
    "                continue                             # Skip if capped\n",
    "            for r2 in others:                        # Iterate cross-sense rows\n",
    "                if not unlimited and partner_count[r1['sent_id']] >= max_partners:  # Re-check during loop\n",
    "                    break                             # Stop if r1 reached cap\n",
    "                try_add_pair(r1, r2, label=0)        # Add negative pair\n",
    "\n",
    "    return pos_pairs, neg_pairs                      # Return both lists\n",
    "\n",
    "\n",
    "# Function: split_and_balance\n",
    "# Purpose : Assign pairs to train/dev/test based on deterministic per-sentence hashing\n",
    "# (both sentences must land in the same split), enforce an optional per-sentence\n",
    "# cap within each split, and balance labels per split if requested.\n",
    "# Inputs  :\n",
    "#   pairs (list): All WiC pair dicts.\n",
    "#   per_sentence_cap (int or None): Cap per sentence inside each split; None disables it.\n",
    "#   global_balance (bool): Whether to balance labels per split.\n",
    "# Returns :\n",
    "#   dict: {'train': [...], 'dev': [...], 'test': [...]} with processed pairs.\n",
    "def split_and_balance(pairs, per_sentence_cap=MAX_PARTNERS_PER_SENT, global_balance=True):  # Split and balance\n",
    "    # keep only pairs whose two sentences hash to the same split\n",
    "    tmp = {'train': [], 'dev': [], 'test': []}       # Containers for intermediate splits\n",
    "    for p in pairs:                                  # Iterate all pairs\n",
    "        b1, b2 = sentence_bin(p['sent_id1']), sentence_bin(p['sent_id2'])  # Compute bins for both sentences\n",
    "        if b1 == b2:                                 # Only keep if both map to the same split\n",
    "            tmp[b1].append(p)                        # Append to that split\n",
    "\n",
    "    # enforce per-split cap (skip if None)\n",
    "    def enforce_split_cap(arr):                      # Local helper to apply per-sentence cap\n",
    "        if per_sentence_cap is None:                 # If cap is disabled\n",
    "            return list(arr)                         # Return all pairs unmodified\n",
    "        counts = Counter()                           # Per-sentence counts inside this split\n",
    "        out = []                                     # Output list after capping\n",
    "        for p in rng.sample(arr, len(arr)):          # Shuffle then greedily keep under caps\n",
    "            a, b = p['sent_id1'], p['sent_id2']      # Sentence IDs\n",
    "            if counts[a] < per_sentence_cap and counts[b] < per_sentence_cap:  # Check both caps\n",
    "                out.append(p)                        # Keep the pair\n",
    "                counts[a] += 1                       # Increment for a\n",
    "                counts[b] += 1                       # Increment for b\n",
    "        return out                                   # Return capped list\n",
    "\n",
    "    for split in tmp:                                # For each split key\n",
    "        tmp[split] = enforce_split_cap(tmp[split])   # Apply per-sentence cap\n",
    "\n",
    "    if global_balance:                               # If balancing is enabled\n",
    "        balanced = {}                                # Output dict for balanced splits\n",
    "        for split in ['train', 'dev', 'test']:       # Process splits in fixed order\n",
    "            arr = tmp[split]                         # Pairs in the current split\n",
    "            pos = [p for p in arr if p['label'] == 1]  # Positive pairs\n",
    "            neg = [p for p in arr if p['label'] == 0]  # Negative pairs\n",
    "            rng.shuffle(pos); rng.shuffle(neg)       # Shuffle class lists\n",
    "            m = min(len(pos), len(neg))              # Target balanced size\n",
    "            balanced[split] = pos[:m] + neg[:m]      # Truncate to balance\n",
    "            rng.shuffle(balanced[split])             # Shuffle final split\n",
    "        return balanced                              # Return balanced splits\n",
    "\n",
    "    return tmp                                       # Return unbalanced splits if balancing disabled\n",
    "\n",
    "\n",
    "# Function: main\n",
    "# Purpose : Orchestrate the full pipeline: load WSD rows, generate capped WiC pairs,\n",
    "# split and balance them, write train/dev/test JSON files, and save summary.\n",
    "# Inputs  :\n",
    "#   uses constants from the config section.\n",
    "# Returns : \n",
    "#   writes files to OUT_DIR and prints a summary.\n",
    "def main():                                          # Program entry point\n",
    "    print(\"Loading WSD\")                             # Status message\n",
    "    df = load_wsd(WSD_CSV)                           # Load and clean WSD dataframe\n",
    "\n",
    "    all_pairs = []                                   # Accumulator for all pairs across lemmas\n",
    "    lemmas = sorted(df['lemma'].unique())            # Unique lemmas in deterministic order\n",
    "    print(f\"Found {len(lemmas)} lemmas.\")            # Report number of lemmas discovered\n",
    "\n",
    "    # build pairs per lemma with per-sentence cap\n",
    "    for lem in lemmas:                               # Iterate each lemma\n",
    "        sub = df[df['lemma'] == lem].copy()          # Subset rows for this lemma\n",
    "        if sub.shape[0] < 2:                         # Skip lemmas with fewer than 2 examples\n",
    "            continue                                 # Proceed to next lemma\n",
    "        pos_pairs, neg_pairs = build_pairs_capped(sub, max_partners=MAX_PARTNERS_PER_SENT)  # Generate pairs\n",
    "        if not pos_pairs and not neg_pairs:          # If no pairs were produced\n",
    "            continue                                 # Proceed to next lemma\n",
    "        pairs = pos_pairs + neg_pairs                # Merge positive and negative pairs\n",
    "        all_pairs.extend(pairs)                      # Accumulate\n",
    "\n",
    "    # split & enforce constraints\n",
    "    splits = split_and_balance(all_pairs, per_sentence_cap=MAX_PARTNERS_PER_SENT, global_balance=GLOBAL_BALANCE)  # Split and balance\n",
    "    train, dev, test = splits['train'], splits['dev'], splits['test']  # Unpack splits\n",
    "\n",
    "    # final shuffle & dump\n",
    "    rng.shuffle(train); rng.shuffle(dev); rng.shuffle(test)  # Shuffle each split before saving\n",
    "\n",
    "    def dump_json(filename, data):                  # Helper to save a split to JSON\n",
    "        path = os.path.join(OUT_DIR, filename)      # Compose output path\n",
    "        with open(path, 'w', encoding='utf-8') as f:  # Open file for writing\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)  # Write JSON with indentation\n",
    "        print(f\"Saved {filename}: {len(data)} pairs\")  # Report save status\n",
    "\n",
    "    dump_json('wic_train.json', train)              # Save training split\n",
    "    dump_json('wic_dev.json',   dev)                # Save development split\n",
    "    dump_json('wic_test.json',  test)               # Save test split\n",
    "\n",
    "    # small summary\n",
    "    def stats(arr):                                  # Helper to compute (pos, neg, pos_ratio)\n",
    "        if not arr: return (0,0,0.0)                 # Handle empty splits\n",
    "        pos = sum(1 for x in arr if x['label']==1)   # Count positives\n",
    "        neg = len(arr) - pos                         # Count negatives\n",
    "        return (pos, neg, round(pos/len(arr), 3))    # Return counts and positive ratio\n",
    "\n",
    "    summary = {                                      # Build summary information\n",
    "        'train': {'total': len(train), 'pos_neg': stats(train)},  # Train totals and balance\n",
    "        'dev':   {'total': len(dev),   'pos_neg': stats(dev)},    # Dev totals and balance\n",
    "        'test':  {'total': len(test),  'pos_neg': stats(test)},   # Test totals and balance\n",
    "        'caps':  {'MAX_PARTNERS_PER_SENT': MAX_PARTNERS_PER_SENT} # Cap configuration\n",
    "    }\n",
    "    with open(os.path.join(OUT_DIR, 'summary.json'), 'w', encoding='utf-8') as f:  # Open summary path\n",
    "        json.dump(summary, f, ensure_ascii=False, indent=2)                         # Write summary JSON\n",
    "    print(\"Summary:\", summary)                    # Print summary to console\n",
    "\n",
    "if __name__ == \"__main__\":                        # Standard Python entry guard\n",
    "    main()                                        # Execute the pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf777a42",
   "metadata": {},
   "source": [
    "**Zero-Shot WiC Evaluator for Bangla Polysemy (No Training) on Capped WIC dataset**\n",
    "\n",
    "This script evaluates Word-in-Context (WiC) pairs in a zero-shot manner.\n",
    "It inserts [TGT]…[/TGT] markers around target spans, encodes each sentence\n",
    "with a chosen multilingual model (SentenceTransformers backend when\n",
    "available; Hugging Face Transformers + mean pooling otherwise), computes\n",
    "cosine similarity between sentence embeddings, calibrates a single\n",
    "similarity threshold on the dev split (maximizing F1), and reports\n",
    "F1/Accuracy on the test split. It also saves per-pair predictions and a\n",
    "run summary for all models listed in MODELS.\n",
    "\n",
    " 1) Load WiC dev/test JSON files.\n",
    " 2) Insert target markers using provided offsets.\n",
    " 3) Encode sentence pairs with the specified backbone.\n",
    " 4) Compute cosine similarity between embeddings.\n",
    " 5) Sweep thresholds on dev to maximize F1 (calibration).\n",
    " 6) Apply the best threshold to test; compute F1/Accuracy.\n",
    " 7) Save predictions and a CSV summary per model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c08ec79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sahajbert | neuropark/sahajBERT\n",
      "Dev : best_threshold=0.8805 | F1=0.7672 | Acc=0.7523\n",
      "Test : F1=0.6354 | Acc=0.6392\n",
      "\n",
      "muril | google/muril-base-cased\n",
      "Dev : best_threshold=0.9955 | F1=0.7480 | Acc=0.7156\n",
      "Test : F1=0.6878 | Acc=0.6443\n",
      "\n",
      "labse | sentence-transformers/LaBSE\n",
      "Dev : best_threshold=0.2792 | F1=0.6730 | Acc=0.5275\n",
      "Test : F1=0.6739 | Acc=0.5361\n",
      "\n",
      "e5 | intfloat/multilingual-e5-base\n",
      "Dev : best_threshold=0.7857 | F1=0.6885 | Acc=0.5642\n",
      "Test : F1=0.6494 | Acc=0.5103\n",
      "\n",
      "banglabert | sagorsarker/bangla-bert-base\n",
      "Dev : best_threshold=0.5664 | F1=0.6974 | Acc=0.5780\n",
      "Test : F1=0.6544 | Acc=0.5155\n",
      "\n",
      "Calibrated zero-shot summary\n",
      "     model   thresh   dev_f1  dev_acc  test_f1  test_acc                                                                                                                                              pred_path\n",
      " sahajbert 0.880528 0.767241 0.752294 0.635417  0.639175  C:\\Users\\Student\\Downloads\\projecting_sentences-main\\projecting_sentences-main\\Result\\zero_shot_result_capped\\sahajbert_zeroshot_test_predictions.csv\n",
      "     muril 0.995497 0.747967 0.715596 0.687783  0.644330      C:\\Users\\Student\\Downloads\\projecting_sentences-main\\projecting_sentences-main\\Result\\zero_shot_result_capped\\muril_zeroshot_test_predictions.csv\n",
      "     labse 0.279192 0.673016 0.527523 0.673913  0.536082      C:\\Users\\Student\\Downloads\\projecting_sentences-main\\projecting_sentences-main\\Result\\zero_shot_result_capped\\labse_zeroshot_test_predictions.csv\n",
      "        e5 0.785747 0.688525 0.564220 0.649446  0.510309         C:\\Users\\Student\\Downloads\\projecting_sentences-main\\projecting_sentences-main\\Result\\zero_shot_result_capped\\e5_zeroshot_test_predictions.csv\n",
      "banglabert 0.566386 0.697368 0.577982 0.654412  0.515464 C:\\Users\\Student\\Downloads\\projecting_sentences-main\\projecting_sentences-main\\Result\\zero_shot_result_capped\\banglabert_zeroshot_test_predictions.csv\n",
      "\n",
      "Saved summary to: C:\\Users\\Student\\Downloads\\projecting_sentences-main\\projecting_sentences-main\\Result\\zero_shot_result_capped\\zeroshot_calibrated_summary.csv\n",
      "\n",
      "Done in 629.1s\n"
     ]
    }
   ],
   "source": [
    "import os                              # OS utilities for paths and directories\n",
    "import json                            # JSON I/O for WiC files and summaries\n",
    "import math                            # math.isclose used in threshold tie-breaking\n",
    "import time                            # simple wall-clock timing\n",
    "from collections import defaultdict    # imported (not strictly used); safe to keep\n",
    "\n",
    "import numpy as np                     # vector ops and cosine components\n",
    "import pandas as pd                    # tabular export of predictions/summary\n",
    "import torch                           # device detection and no-grad inference\n",
    "from sklearn.metrics import f1_score, accuracy_score  # evaluation metrics\n",
    "\n",
    "# Backends\n",
    "from sentence_transformers import SentenceTransformer  # ST models (LaBSE/e5)\n",
    "from transformers import AutoTokenizer, AutoModel      # HF base models\n",
    "\n",
    "# Configuration\n",
    "WIC_DEV  = r'C:\\Users\\Student\\Downloads\\projecting_sentences-main\\projecting_sentences-main\\Dataset\\bangla_wic_dataset_capped\\wic_dev.json'   # path to dev WiC JSON\n",
    "WIC_TEST = r'C:\\Users\\Student\\Downloads\\projecting_sentences-main\\projecting_sentences-main\\Dataset\\bangla_wic_dataset_capped\\wic_test.json'  # path to test WiC JSON\n",
    "\n",
    "OUT_DIR = r'C:\\Users\\Student\\Downloads\\projecting_sentences-main\\projecting_sentences-main\\Result\\zero_shot_result_capped'    # output directory\n",
    "os.makedirs(OUT_DIR, exist_ok=True)   # ensure output directory exists\n",
    "\n",
    "MODELS = [\n",
    "    ('sahajbert',  'neuropark/sahajBERT'),               # HF base model -> Transformers mean pooling\n",
    "    ('muril',      'google/muril-base-cased'),           # HF base model -> Transformers mean pooling\n",
    "    ('labse',      'sentence-transformers/LaBSE'),       # ST model      -> SentenceTransformer\n",
    "    ('e5',         'intfloat/multilingual-e5-base'),     # ST model      -> SentenceTransformer (with \"query:\" prefix)\n",
    "    ('banglabert', 'sagorsarker/bangla-bert-base'),      # HF base model -> Transformers mean pooling\n",
    "]  # list of (short_name, HF model id)\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'  # pick GPU if available\n",
    "BATCH_SIZE = 64                                          # batch size for encoding\n",
    "MAX_LEN = 256                                            # max sequence length for HF tokenizers\n",
    "\n",
    "# Use the same target markers as Code 2\n",
    "L_MARK, R_MARK = '[TGT]', '[/TGT]'                       # target span markers\n",
    "\n",
    "\n",
    "# Function: maybe_prefix\n",
    "# Purpose : Add the \"query:\" prefix required by e5 models (kept symmetric on both sides).\n",
    "# Inputs  : \n",
    "#   model_id (str) -> HF repo id; \n",
    "#   text (str) -> input sentence (with markers).\n",
    "# Outputs : \n",
    "#   (str) -> possibly prefixed text.\n",
    "def maybe_prefix(model_id, text):\n",
    "    return f\"query: {text}\" if 'e5' in model_id.lower() else text  # add \"query:\" for e5; no change otherwise\n",
    "\n",
    "\n",
    "# Helpers\n",
    "\n",
    "# Function: load_wic\n",
    "# Purpose : Load a WiC JSON file into a Python list of dicts.\n",
    "# Inputs  : \n",
    "#   path (str) -> filesystem path to a WiC JSON file.\n",
    "# Outputs : \n",
    "#   (list[dict]) -> each dict contains WiC fields including sentences,\n",
    "#   offsets, labels, and IDs.\n",
    "def load_wic(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:  # open the JSON file with UTF-8\n",
    "        return json.load(f)                       # parse and return as Python objects\n",
    "\n",
    "\n",
    "# Function: insert_markers\n",
    "# Purpose : Surround the target span in a sentence with [TGT]…[/TGT] based on offsets.\n",
    "# Inputs  : \n",
    "#   text (str) -> sentence string;\n",
    "#   start (int), end (int) -> character offsets (start inclusive, end exclusive);\n",
    "#   l_mark (str), r_mark (str) -> left/right marker tokens.\n",
    "# Outputs : \n",
    "#   (str) -> sentence with markers inserted or original text if offsets invalid.\n",
    "def insert_markers(text, start, end, l_mark=L_MARK, r_mark=R_MARK):\n",
    "    \"\"\"Insert [TGT] .. [/TGT] around the target span; fallback to raw text if indices invalid.\"\"\"\n",
    "    try:                                                # bounds safety\n",
    "        if 0 <= start <= end <= len(text):              # ensure valid offsets\n",
    "            return text[:start] + l_mark + text[start:end] + r_mark + text[end:]  # splice in markers\n",
    "    except Exception:                                   # any unexpected error falls back\n",
    "        pass\n",
    "    return text                                         # fallback: return original sentence\n",
    "\n",
    "\n",
    "# Function: cosine_sim\n",
    "# Purpose : Compute cosine similarity for aligned rows of two embedding arrays.\n",
    "# Inputs  : \n",
    "#   a (np.ndarray), b (np.ndarray) -> shape [N, D] embeddings.\n",
    "# Outputs : \n",
    "#   (np.ndarray) -> shape [N] array of cosine similarities.\n",
    "def cosine_sim(a, b):\n",
    "    a = np.asarray(a, dtype=np.float32)                            # cast to float32\n",
    "    b = np.asarray(b, dtype=np.float32)                            # cast to float32\n",
    "    a_norm = a / (np.linalg.norm(a, axis=1, keepdims=True) + 1e-12)  # row-wise L2 normalize\n",
    "    b_norm = b / (np.linalg.norm(b, axis=1, keepdims=True) + 1e-12)  # row-wise L2 normalize\n",
    "    return np.sum(a_norm * b_norm, axis=1)                         # cosine = dot of normalized rows\n",
    "\n",
    "\n",
    "# Function: best_threshold\n",
    "# Purpose : Select the similarity threshold that maximizes F1 on dev, tie-break by accuracy.\n",
    "# Inputs  : \n",
    "#   sims (array-like[float]) -> similarity scores; labels (array-like[int]) -> gold 0/1.\n",
    "# Outputs : \n",
    "#   (tuple) -> (best_threshold: float, best_f1: float, best_acc: float).\n",
    "def best_threshold(sims, labels):\n",
    "    \"\"\"Pick the similarity threshold that maximizes F1 on dev (break ties by higher accuracy).\"\"\"\n",
    "    sims = np.asarray(sims, dtype=float)                     # vectorize similarities\n",
    "    y = np.asarray(labels, dtype=int)                        # vectorize labels\n",
    "    uniq = np.unique(sims)                                   # distinct sims to form sweep points\n",
    "    if len(uniq) == 1:                                       # degenerate: all identical scores\n",
    "        t_candidates = [uniq[0]]                             # only that one threshold\n",
    "    else:\n",
    "        mids = (uniq[:-1] + uniq[1:]) / 2.0                  # midpoints between sorted neighbors\n",
    "        t_candidates = [uniq[0]-1e-6] + list(mids) + [uniq[-1]+1e-6]  # include small margins\n",
    "    best_t, best_f1, best_acc = None, -1.0, -1.0             # initialize bests\n",
    "    for t in t_candidates:                                   # sweep thresholds\n",
    "        pred = (sims >= t).astype(int)                       # predict same-sense if sim ≥ t\n",
    "        f1  = f1_score(y, pred)                              # compute F1\n",
    "        acc = accuracy_score(y, pred)                        # compute Accuracy\n",
    "        if (f1 > best_f1) or (math.isclose(f1, best_f1) and acc > best_acc):  # tie-break by Acc\n",
    "            best_t, best_f1, best_acc = float(t), float(f1), float(acc)       # store new best\n",
    "    return best_t, best_f1, best_acc                         # return optimal threshold and scores\n",
    "\n",
    "\n",
    "\n",
    "# Unified encoding backend (ST for ST repos, HF+mean-pooling otherwise)\n",
    "_st_cache = {}                                               # cache for SentenceTransformer models\n",
    "_hf_cache = {}                                               # cache for (tokenizer, HF model) tuples\n",
    "\n",
    "\n",
    "# Function: _is_sentence_transformers_repo\n",
    "# Purpose : Decide whether to use SentenceTransformers or HF+mean pooling for a repo id.\n",
    "# Inputs  : \n",
    "#   model_id (str) -> HF repository identifier.\n",
    "# Outputs : \n",
    "#   (bool) -> True if SentenceTransformers API should be used.\n",
    "def _is_sentence_transformers_repo(model_id: str) -> bool:\n",
    "    mid = model_id.lower()                                   # normalize case\n",
    "    return ('sentence-transformers/' in mid) or ('/e5' in mid) or mid.startswith('intfloat/')  # heuristic\n",
    "\n",
    "\n",
    "# Function: _get_st_encoder\n",
    "# Purpose : Lazy-load and cache a SentenceTransformer encoder callable.\n",
    "# Inputs  : \n",
    "#   model_id (str) -> SentenceTransformers-compatible repo id.\n",
    "# Outputs : \n",
    "#   (callable) -> encode(texts: list[str], batch_size: int) -> np.ndarray [N, D]\n",
    "def _get_st_encoder(model_id):\n",
    "    if model_id not in _st_cache:                                            # if not cached\n",
    "        _st_cache[model_id] = SentenceTransformer(model_id, device=DEVICE)   # load ST model\n",
    "    st_model = _st_cache[model_id]                                          # fetch cached model\n",
    "    def encode(texts, batch_size=BATCH_SIZE):                               # encoder closure\n",
    "        with torch.inference_mode():                                        # no gradients\n",
    "            return st_model.encode(                                         # SentenceTransformers encode\n",
    "                texts,\n",
    "                batch_size=batch_size,\n",
    "                convert_to_numpy=True,\n",
    "                show_progress_bar=False,\n",
    "                normalize_embeddings=False  # leave cosine normalization to cosine_sim()\n",
    "            )\n",
    "    return encode                                                            # return callable\n",
    "\n",
    "\n",
    "# Function: _get_hf_encoder\n",
    "# Purpose : Lazy-load and cache a Hugging Face base model + tokenizer, returning\n",
    "# a callable that encodes texts via mean pooling of last hidden states.\n",
    "# Inputs  : \n",
    "#   model_id (str) -> Hugging Face repo id (non-ST).\n",
    "# Outputs : \n",
    "#   (callable) -> encode(texts: list[str], batch_size: int, max_length: int) -> np.ndarray [N, D]\n",
    "def _get_hf_encoder(model_id):\n",
    "    if model_id not in _hf_cache:                                           # if not cached\n",
    "        tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)        # tokenizer\n",
    "        mdl = AutoModel.from_pretrained(model_id).to(DEVICE)                # base model\n",
    "        mdl.eval()                                                          # inference mode\n",
    "        _hf_cache[model_id] = (tok, mdl)                                    # cache tuple\n",
    "    tok, mdl = _hf_cache[model_id]                                          # unpack cache\n",
    "\n",
    "    def mean_pool(last_hidden_state, attention_mask):                       # pooling helper\n",
    "        mask = attention_mask.unsqueeze(-1)                                 # [B,T,1] expand mask\n",
    "        summed = (last_hidden_state * mask).sum(dim=1)                      # sum masked states\n",
    "        counts = mask.sum(dim=1).clamp(min=1e-9)                            # token counts per row\n",
    "        return (summed / counts)                                            # mean = sum / count\n",
    "\n",
    "    def encode(texts, batch_size=BATCH_SIZE, max_length=MAX_LEN):           # encoder closure\n",
    "        embs = []                                                           # accumulator\n",
    "        with torch.inference_mode():                                        # no gradients\n",
    "            for i in range(0, len(texts), batch_size):                      # mini-batches\n",
    "                batch = texts[i:i+batch_size]                               # slice texts\n",
    "                inputs = tok(                                               # tokenize\n",
    "                    batch, padding=True, truncation=True,\n",
    "                    max_length=max_length, return_tensors='pt'\n",
    "                ).to(DEVICE)\n",
    "                outputs = mdl(**inputs)                                     # forward pass\n",
    "                pooled = mean_pool(outputs.last_hidden_state, inputs['attention_mask'])  # mean pool\n",
    "                embs.append(pooled.detach().cpu().numpy())                  # to CPU numpy\n",
    "        return np.vstack(embs)                                              # stack to [N, D]\n",
    "    return encode                                                           # return callable\n",
    "\n",
    "\n",
    "# Function: get_encoder\n",
    "# Purpose : Factory that returns a text->embedding encoder function for a repo id.\n",
    "# Inputs  : \n",
    "#   model_id (str) -> HF repository identifier.\n",
    "# Outputs : \n",
    "#   (callable) -> encode(texts: list[str], batch_size: int, [max_length]) -> np.ndarray [N, D]\n",
    "def get_encoder(model_id):\n",
    "    return _get_st_encoder(model_id) if _is_sentence_transformers_repo(model_id) else _get_hf_encoder(model_id)  # pick backend\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "# Function: eval_model\n",
    "# Purpose : End-to-end calibrated zero-shot evaluation for one backbone.\n",
    "# Inputs  : \n",
    "#   model_name (str) -> short label for reports/filenames;\n",
    "#   model_id (str)   -> HF repo id;\n",
    "#   dev_data (list[dict])  -> WiC dev examples;\n",
    "#   test_data (list[dict]) -> WiC test examples;\n",
    "#   out_dir (str)     -> directory to write predictions.\n",
    "# Outputs : \n",
    "#   (dict) -> summary with threshold, dev/test F1/Acc, and path to predictions CSV.\n",
    "def eval_model(model_name, model_id, dev_data, test_data, out_dir=OUT_DIR):\n",
    "    \"\"\"\n",
    "    Calibrated zero-shot:\n",
    "      - insert [TGT]…[/TGT] around spans (same as Code 2’s logic),\n",
    "      - encode each side with the same backbone as Code 2,\n",
    "      - cosine similarity,\n",
    "      - choose threshold on dev (max F1),\n",
    "      - evaluate on test,\n",
    "      - save predictions and a summary row.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{model_name} | {model_id}\")          # header for the current model\n",
    "    encoder = get_encoder(model_id)                         # obtain encoding callable for this repo\n",
    "\n",
    "    # DEV\n",
    "    dev_left, dev_right, dev_labels = [], [], []           # containers for left/right texts and labels\n",
    "    for ex in dev_data:                                     # iterate dev examples\n",
    "        s1 = insert_markers(ex['sentence1'], ex['start1'], ex['end1'])  # mark target in sentence1\n",
    "        s2 = insert_markers(ex['sentence2'], ex['start2'], ex['end2'])  # mark target in sentence2\n",
    "        dev_left.append(maybe_prefix(model_id, s1))         # prefix (e5) or leave as is\n",
    "        dev_right.append(maybe_prefix(model_id, s2))        # prefix (e5) or leave as is\n",
    "        dev_labels.append(int(ex['label']))                 # store gold label\n",
    "\n",
    "    dev_emb1 = encoder(dev_left)                            # encode left dev sentences\n",
    "    dev_emb2 = encoder(dev_right)                           # encode right dev sentences\n",
    "    dev_sims = cosine_sim(dev_emb1, dev_emb2)               # cosine similarities for dev\n",
    "    thr, dev_f1, dev_acc = best_threshold(dev_sims, dev_labels)  # pick best threshold on dev\n",
    "    print(f\"Dev : best_threshold={thr:.4f} | F1={dev_f1:.4f} | Acc={dev_acc:.4f}\")  # report dev calibration\n",
    "\n",
    "    # TEST\n",
    "    test_left, test_right, test_labels = [], [], []         # prepare test containers\n",
    "    for ex in test_data:                                    # iterate test examples\n",
    "        s1 = insert_markers(ex['sentence1'], ex['start1'], ex['end1'])  # mark sentence1\n",
    "        s2 = insert_markers(ex['sentence2'], ex['start2'], ex['end2'])  # mark sentence2\n",
    "        test_left.append(maybe_prefix(model_id, s1))        # prefix if e5\n",
    "        test_right.append(maybe_prefix(model_id, s2))       # prefix if e5\n",
    "        test_labels.append(int(ex['label']))                # gold label\n",
    "\n",
    "    test_emb1 = encoder(test_left)                          # encode left test sentences\n",
    "    test_emb2 = encoder(test_right)                         # encode right test sentences\n",
    "    test_sims = cosine_sim(test_emb1, test_emb2)            # cosine similarities\n",
    "    test_pred = (test_sims >= thr).astype(int)              # predict using calibrated threshold\n",
    "\n",
    "    test_f1  = f1_score(test_labels, test_pred)             # test F1\n",
    "    test_acc = accuracy_score(test_labels, test_pred)       # test Accuracy\n",
    "    print(f\"Test : F1={test_f1:.4f} | Acc={test_acc:.4f}\")  # report test metrics\n",
    "\n",
    "    # Save per-pair predictions\n",
    "    rows = []                                               # per-pair rows to write\n",
    "    for ex, sim, pred in zip(test_data, test_sims, test_pred):  # iterate results\n",
    "        rows.append({\n",
    "            'model': model_name,                            # short model name\n",
    "            'lemma': ex.get('lemma', ''),                   # lemma (if present)\n",
    "            'sent_id1': ex.get('sent_id1', ''),             # sentence id 1\n",
    "            'sent_id2': ex.get('sent_id2', ''),             # sentence id 2\n",
    "            'sim': float(sim),                              # cosine similarity\n",
    "            'pred': int(pred),                              # predicted label\n",
    "            'label': int(ex.get('label', 0)),               # gold label (default 0 if missing)\n",
    "        })\n",
    "    pred_path = os.path.join(out_dir, f'{model_name}_zeroshot_test_predictions.csv')  # CSV path\n",
    "    pd.DataFrame(rows).to_csv(pred_path, index=False, encoding='utf-8-sig')          # write CSV with BOM\n",
    "\n",
    "    return {\n",
    "        'model': model_name,                                # echo model name\n",
    "        'thresh': thr,                                      # chosen threshold\n",
    "        'dev_f1': dev_f1, 'dev_acc': dev_acc,               # dev metrics\n",
    "        'test_f1': test_f1, 'test_acc': test_acc,           # test metrics\n",
    "        'pred_path': pred_path                              # where predictions were saved\n",
    "    }\n",
    "\n",
    "\n",
    "# Function: main\n",
    "# Purpose : Orchestrate the full zero-shot evaluation across configured models.\n",
    "# Inputs  : \n",
    "#   None (uses global config for paths/models).\n",
    "# Outputs : \n",
    "#   None (prints metrics, writes predictions and a summary CSV).\n",
    "def main():\n",
    "    dev_data  = load_wic(WIC_DEV)                           # load dev split\n",
    "    test_data = load_wic(WIC_TEST)                          # load test split\n",
    "\n",
    "    summaries = []                                          # accumulate per-model summaries\n",
    "    t0 = time.time()                                        # start timer\n",
    "    for name, mid in MODELS:                                # iterate configured models\n",
    "        try:\n",
    "            s = eval_model(name, mid, dev_data, test_data, out_dir=OUT_DIR)  # evaluate one model\n",
    "            summaries.append(s)                             # store summary\n",
    "        except Exception as e:                              # robust loop: continue on error\n",
    "            print(f\"[WARN] {name} failed: {e}\")             # report failure\n",
    "\n",
    "    if summaries:                                           # if we collected any results\n",
    "        sum_df = pd.DataFrame(summaries)                    # tabularize summaries\n",
    "        sum_csv = os.path.join(OUT_DIR, 'zeroshot_calibrated_summary.csv')  # summary path\n",
    "        sum_df.to_csv(sum_csv, index=False, encoding='utf-8-sig')           # write summary CSV\n",
    "        print(\"\\nCalibrated zero-shot summary\")       # header\n",
    "        print(sum_df.to_string(index=False))                # pretty-print table\n",
    "        print(f\"\\nSaved summary to: {sum_csv}\")             # location info\n",
    "    print(f\"\\nDone in {time.time()-t0:.1f}s\")               # total elapsed time\n",
    "\n",
    "if __name__ == '__main__':                                  # script entry point\n",
    "    main()                                                  # run main\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3f805a",
   "metadata": {},
   "source": [
    "**Zero-Shot WiC Evaluator for Bangla Polysemy (No Training) on Uncapped WIC dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f542d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sahajbert | neuropark/sahajBERT\n",
      "Dev : best_threshold=0.8781 | F1=0.7309 | Acc=0.7092\n",
      "Test : F1=0.7008 | Acc=0.6740\n",
      "\n",
      "muril | google/muril-base-cased\n",
      "Dev : best_threshold=0.9957 | F1=0.7140 | Acc=0.6805\n",
      "Test : F1=0.6975 | Acc=0.6541\n",
      "\n",
      "labse | sentence-transformers/LaBSE\n",
      "Dev : best_threshold=0.2902 | F1=0.6737 | Acc=0.5350\n",
      "Test : F1=0.6627 | Acc=0.5206\n",
      "\n",
      "e5 | intfloat/multilingual-e5-base\n",
      "Dev : best_threshold=0.7939 | F1=0.6753 | Acc=0.5545\n",
      "Test : F1=0.6690 | Acc=0.5412\n",
      "\n",
      "banglabert | sagorsarker/bangla-bert-base\n",
      "Dev : best_threshold=0.6041 | F1=0.6725 | Acc=0.5602\n",
      "Test : F1=0.6715 | Acc=0.5614\n",
      "\n",
      "Calibrated zero-shot summary\n",
      "     model   thresh   dev_f1  dev_acc  test_f1  test_acc                                                                                                                                                pred_path\n",
      " sahajbert 0.878064 0.730913 0.709164 0.700774  0.674005  C:\\Users\\Student\\Downloads\\projecting_sentences-main\\projecting_sentences-main\\Result\\zero_shot_result_uncapped\\sahajbert_zeroshot_test_predictions.csv\n",
      "     muril 0.995708 0.714014 0.680526 0.697522  0.654098      C:\\Users\\Student\\Downloads\\projecting_sentences-main\\projecting_sentences-main\\Result\\zero_shot_result_uncapped\\muril_zeroshot_test_predictions.csv\n",
      "     labse 0.290190 0.673712 0.535002 0.662712  0.520609      C:\\Users\\Student\\Downloads\\projecting_sentences-main\\projecting_sentences-main\\Result\\zero_shot_result_uncapped\\labse_zeroshot_test_predictions.csv\n",
      "        e5 0.793892 0.675325 0.554518 0.669032  0.541218         C:\\Users\\Student\\Downloads\\projecting_sentences-main\\projecting_sentences-main\\Result\\zero_shot_result_uncapped\\e5_zeroshot_test_predictions.csv\n",
      "banglabert 0.604145 0.672460 0.560246 0.671461  0.561358 C:\\Users\\Student\\Downloads\\projecting_sentences-main\\projecting_sentences-main\\Result\\zero_shot_result_uncapped\\banglabert_zeroshot_test_predictions.csv\n",
      "\n",
      "Saved summary to: C:\\Users\\Student\\Downloads\\projecting_sentences-main\\projecting_sentences-main\\Result\\zero_shot_result_uncapped\\zeroshot_calibrated_summary.csv\n",
      "\n",
      "Done in 9316.9s\n"
     ]
    }
   ],
   "source": [
    "import os                              # OS utilities for paths and directories\n",
    "import json                            # JSON I/O for WiC files and summaries\n",
    "import math                            # math.isclose used in threshold tie-breaking\n",
    "import time                            # simple wall-clock timing\n",
    "from collections import defaultdict    # imported (not strictly used); safe to keep\n",
    "\n",
    "import numpy as np                     # vector ops and cosine components\n",
    "import pandas as pd                    # tabular export of predictions/summary\n",
    "import torch                           # device detection and no-grad inference\n",
    "from sklearn.metrics import f1_score, accuracy_score  # evaluation metrics\n",
    "\n",
    "# Backends\n",
    "from sentence_transformers import SentenceTransformer  # ST models (LaBSE/e5)\n",
    "from transformers import AutoTokenizer, AutoModel      # HF base models\n",
    "\n",
    "# Configuration\n",
    "WIC_DEV  = r'C:\\Users\\Student\\Downloads\\projecting_sentences-main\\projecting_sentences-main\\Dataset\\bangla_wic_dataset_uncapped\\wic_dev.json'   # path to dev WiC JSON\n",
    "WIC_TEST = r'C:\\Users\\Student\\Downloads\\projecting_sentences-main\\projecting_sentences-main\\Dataset\\bangla_wic_dataset_uncapped\\wic_test.json'  # path to test WiC JSON\n",
    "\n",
    "OUT_DIR = r'C:\\Users\\Student\\Downloads\\projecting_sentences-main\\projecting_sentences-main\\Result\\zero_shot_result_uncapped'    # output directory\n",
    "os.makedirs(OUT_DIR, exist_ok=True)   # ensure output directory exists\n",
    "\n",
    "MODELS = [\n",
    "    ('sahajbert',  'neuropark/sahajBERT'),               # HF base model -> Transformers mean pooling\n",
    "    ('muril',      'google/muril-base-cased'),           # HF base model -> Transformers mean pooling\n",
    "    ('labse',      'sentence-transformers/LaBSE'),       # ST model      -> SentenceTransformer\n",
    "    ('e5',         'intfloat/multilingual-e5-base'),     # ST model      -> SentenceTransformer (with \"query:\" prefix)\n",
    "    ('banglabert', 'sagorsarker/bangla-bert-base'),      # HF base model -> Transformers mean pooling\n",
    "]  # list of (short_name, HF model id)\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'  # pick GPU if available\n",
    "BATCH_SIZE = 64                                          # batch size for encoding\n",
    "MAX_LEN = 256                                            # max sequence length for HF tokenizers\n",
    "\n",
    "# Use the same target markers as Code 2\n",
    "L_MARK, R_MARK = '[TGT]', '[/TGT]'                       # target span markers\n",
    "\n",
    "\n",
    "# Function: maybe_prefix\n",
    "# Purpose : Add the \"query:\" prefix required by e5 models (kept symmetric on both sides).\n",
    "# Inputs  : \n",
    "#   model_id (str) -> HF repo id; \n",
    "#   text (str) -> input sentence (with markers).\n",
    "# Outputs : \n",
    "#   (str) -> possibly prefixed text.\n",
    "def maybe_prefix(model_id, text):\n",
    "    return f\"query: {text}\" if 'e5' in model_id.lower() else text  # add \"query:\" for e5; no change otherwise\n",
    "\n",
    "\n",
    "# Helpers\n",
    "\n",
    "# Function: load_wic\n",
    "# Purpose : Load a WiC JSON file into a Python list of dicts.\n",
    "# Inputs  : \n",
    "#   path (str) -> filesystem path to a WiC JSON file.\n",
    "# Outputs : \n",
    "#   (list[dict]) -> each dict contains WiC fields including sentences,\n",
    "#   offsets, labels, and IDs.\n",
    "def load_wic(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:  # open the JSON file with UTF-8\n",
    "        return json.load(f)                       # parse and return as Python objects\n",
    "\n",
    "\n",
    "# Function: insert_markers\n",
    "# Purpose : Surround the target span in a sentence with [TGT]…[/TGT] based on offsets.\n",
    "# Inputs  : \n",
    "#   text (str) -> sentence string;\n",
    "#   start (int), end (int) -> character offsets (start inclusive, end exclusive);\n",
    "#   l_mark (str), r_mark (str) -> left/right marker tokens.\n",
    "# Outputs : \n",
    "#   (str) -> sentence with markers inserted or original text if offsets invalid.\n",
    "def insert_markers(text, start, end, l_mark=L_MARK, r_mark=R_MARK):\n",
    "    \"\"\"Insert [TGT] .. [/TGT] around the target span; fallback to raw text if indices invalid.\"\"\"\n",
    "    try:                                                # bounds safety\n",
    "        if 0 <= start <= end <= len(text):              # ensure valid offsets\n",
    "            return text[:start] + l_mark + text[start:end] + r_mark + text[end:]  # splice in markers\n",
    "    except Exception:                                   # any unexpected error falls back\n",
    "        pass\n",
    "    return text                                         # fallback: return original sentence\n",
    "\n",
    "\n",
    "# Function: cosine_sim\n",
    "# Purpose : Compute cosine similarity for aligned rows of two embedding arrays.\n",
    "# Inputs  : \n",
    "#   a (np.ndarray), b (np.ndarray) -> shape [N, D] embeddings.\n",
    "# Outputs : \n",
    "#   (np.ndarray) -> shape [N] array of cosine similarities.\n",
    "def cosine_sim(a, b):\n",
    "    a = np.asarray(a, dtype=np.float32)                            # cast to float32\n",
    "    b = np.asarray(b, dtype=np.float32)                            # cast to float32\n",
    "    a_norm = a / (np.linalg.norm(a, axis=1, keepdims=True) + 1e-12)  # row-wise L2 normalize\n",
    "    b_norm = b / (np.linalg.norm(b, axis=1, keepdims=True) + 1e-12)  # row-wise L2 normalize\n",
    "    return np.sum(a_norm * b_norm, axis=1)                         # cosine = dot of normalized rows\n",
    "\n",
    "\n",
    "# Function: best_threshold\n",
    "# Purpose : Select the similarity threshold that maximizes F1 on dev, tie-break by accuracy.\n",
    "# Inputs  : \n",
    "#   sims (array-like[float]) -> similarity scores; labels (array-like[int]) -> gold 0/1.\n",
    "# Outputs : \n",
    "#   (tuple) -> (best_threshold: float, best_f1: float, best_acc: float).\n",
    "def best_threshold(sims, labels):\n",
    "    \"\"\"Pick the similarity threshold that maximizes F1 on dev (break ties by higher accuracy).\"\"\"\n",
    "    sims = np.asarray(sims, dtype=float)                     # vectorize similarities\n",
    "    y = np.asarray(labels, dtype=int)                        # vectorize labels\n",
    "    uniq = np.unique(sims)                                   # distinct sims to form sweep points\n",
    "    if len(uniq) == 1:                                       # degenerate: all identical scores\n",
    "        t_candidates = [uniq[0]]                             # only that one threshold\n",
    "    else:\n",
    "        mids = (uniq[:-1] + uniq[1:]) / 2.0                  # midpoints between sorted neighbors\n",
    "        t_candidates = [uniq[0]-1e-6] + list(mids) + [uniq[-1]+1e-6]  # include small margins\n",
    "    best_t, best_f1, best_acc = None, -1.0, -1.0             # initialize bests\n",
    "    for t in t_candidates:                                   # sweep thresholds\n",
    "        pred = (sims >= t).astype(int)                       # predict same-sense if sim ≥ t\n",
    "        f1  = f1_score(y, pred)                              # compute F1\n",
    "        acc = accuracy_score(y, pred)                        # compute Accuracy\n",
    "        if (f1 > best_f1) or (math.isclose(f1, best_f1) and acc > best_acc):  # tie-break by Acc\n",
    "            best_t, best_f1, best_acc = float(t), float(f1), float(acc)       # store new best\n",
    "    return best_t, best_f1, best_acc                         # return optimal threshold and scores\n",
    "\n",
    "\n",
    "\n",
    "# Unified encoding backend (ST for ST repos, HF+mean-pooling otherwise)\n",
    "_st_cache = {}                                               # cache for SentenceTransformer models\n",
    "_hf_cache = {}                                               # cache for (tokenizer, HF model) tuples\n",
    "\n",
    "\n",
    "# Function: _is_sentence_transformers_repo\n",
    "# Purpose : Decide whether to use SentenceTransformers or HF+mean pooling for a repo id.\n",
    "# Inputs  : \n",
    "#   model_id (str) -> HF repository identifier.\n",
    "# Outputs : \n",
    "#   (bool) -> True if SentenceTransformers API should be used.\n",
    "def _is_sentence_transformers_repo(model_id: str) -> bool:\n",
    "    mid = model_id.lower()                                   # normalize case\n",
    "    return ('sentence-transformers/' in mid) or ('/e5' in mid) or mid.startswith('intfloat/')  # heuristic\n",
    "\n",
    "\n",
    "# Function: _get_st_encoder\n",
    "# Purpose : Lazy-load and cache a SentenceTransformer encoder callable.\n",
    "# Inputs  : \n",
    "#   model_id (str) -> SentenceTransformers-compatible repo id.\n",
    "# Outputs : \n",
    "#   (callable) -> encode(texts: list[str], batch_size: int) -> np.ndarray [N, D]\n",
    "def _get_st_encoder(model_id):\n",
    "    if model_id not in _st_cache:                                            # if not cached\n",
    "        _st_cache[model_id] = SentenceTransformer(model_id, device=DEVICE)   # load ST model\n",
    "    st_model = _st_cache[model_id]                                          # fetch cached model\n",
    "    def encode(texts, batch_size=BATCH_SIZE):                               # encoder closure\n",
    "        with torch.inference_mode():                                        # no gradients\n",
    "            return st_model.encode(                                         # SentenceTransformers encode\n",
    "                texts,\n",
    "                batch_size=batch_size,\n",
    "                convert_to_numpy=True,\n",
    "                show_progress_bar=False,\n",
    "                normalize_embeddings=False  # leave cosine normalization to cosine_sim()\n",
    "            )\n",
    "    return encode                                                            # return callable\n",
    "\n",
    "\n",
    "# Function: _get_hf_encoder\n",
    "# Purpose : Lazy-load and cache a Hugging Face base model + tokenizer, returning\n",
    "# a callable that encodes texts via mean pooling of last hidden states.\n",
    "# Inputs  : \n",
    "#   model_id (str) -> Hugging Face repo id (non-ST).\n",
    "# Outputs : \n",
    "#   (callable) -> encode(texts: list[str], batch_size: int, max_length: int) -> np.ndarray [N, D]\n",
    "def _get_hf_encoder(model_id):\n",
    "    if model_id not in _hf_cache:                                           # if not cached\n",
    "        tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)        # tokenizer\n",
    "        mdl = AutoModel.from_pretrained(model_id).to(DEVICE)                # base model\n",
    "        mdl.eval()                                                          # inference mode\n",
    "        _hf_cache[model_id] = (tok, mdl)                                    # cache tuple\n",
    "    tok, mdl = _hf_cache[model_id]                                          # unpack cache\n",
    "\n",
    "    def mean_pool(last_hidden_state, attention_mask):                       # pooling helper\n",
    "        mask = attention_mask.unsqueeze(-1)                                 # [B,T,1] expand mask\n",
    "        summed = (last_hidden_state * mask).sum(dim=1)                      # sum masked states\n",
    "        counts = mask.sum(dim=1).clamp(min=1e-9)                            # token counts per row\n",
    "        return (summed / counts)                                            # mean = sum / count\n",
    "\n",
    "    def encode(texts, batch_size=BATCH_SIZE, max_length=MAX_LEN):           # encoder closure\n",
    "        embs = []                                                           # accumulator\n",
    "        with torch.inference_mode():                                        # no gradients\n",
    "            for i in range(0, len(texts), batch_size):                      # mini-batches\n",
    "                batch = texts[i:i+batch_size]                               # slice texts\n",
    "                inputs = tok(                                               # tokenize\n",
    "                    batch, padding=True, truncation=True,\n",
    "                    max_length=max_length, return_tensors='pt'\n",
    "                ).to(DEVICE)\n",
    "                outputs = mdl(**inputs)                                     # forward pass\n",
    "                pooled = mean_pool(outputs.last_hidden_state, inputs['attention_mask'])  # mean pool\n",
    "                embs.append(pooled.detach().cpu().numpy())                  # to CPU numpy\n",
    "        return np.vstack(embs)                                              # stack to [N, D]\n",
    "    return encode                                                           # return callable\n",
    "\n",
    "\n",
    "# Function: get_encoder\n",
    "# Purpose : Factory that returns a text->embedding encoder function for a repo id.\n",
    "# Inputs  : \n",
    "#   model_id (str) -> HF repository identifier.\n",
    "# Outputs : \n",
    "#   (callable) -> encode(texts: list[str], batch_size: int, [max_length]) -> np.ndarray [N, D]\n",
    "def get_encoder(model_id):\n",
    "    return _get_st_encoder(model_id) if _is_sentence_transformers_repo(model_id) else _get_hf_encoder(model_id)  # pick backend\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "# Function: eval_model\n",
    "# Purpose : End-to-end calibrated zero-shot evaluation for one backbone.\n",
    "# Inputs  : \n",
    "#   model_name (str) -> short label for reports/filenames;\n",
    "#   model_id (str)   -> HF repo id;\n",
    "#   dev_data (list[dict])  -> WiC dev examples;\n",
    "#   test_data (list[dict]) -> WiC test examples;\n",
    "#   out_dir (str)     -> directory to write predictions.\n",
    "# Outputs : \n",
    "#   (dict) -> summary with threshold, dev/test F1/Acc, and path to predictions CSV.\n",
    "def eval_model(model_name, model_id, dev_data, test_data, out_dir=OUT_DIR):\n",
    "    \"\"\"\n",
    "    Calibrated zero-shot:\n",
    "      - insert [TGT]…[/TGT] around spans (same as Code 2’s logic),\n",
    "      - encode each side with the same backbone as Code 2,\n",
    "      - cosine similarity,\n",
    "      - choose threshold on dev (max F1),\n",
    "      - evaluate on test,\n",
    "      - save predictions and a summary row.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{model_name} | {model_id}\")          # header for the current model\n",
    "    encoder = get_encoder(model_id)                         # obtain encoding callable for this repo\n",
    "\n",
    "    # DEV\n",
    "    dev_left, dev_right, dev_labels = [], [], []           # containers for left/right texts and labels\n",
    "    for ex in dev_data:                                     # iterate dev examples\n",
    "        s1 = insert_markers(ex['sentence1'], ex['start1'], ex['end1'])  # mark target in sentence1\n",
    "        s2 = insert_markers(ex['sentence2'], ex['start2'], ex['end2'])  # mark target in sentence2\n",
    "        dev_left.append(maybe_prefix(model_id, s1))         # prefix (e5) or leave as is\n",
    "        dev_right.append(maybe_prefix(model_id, s2))        # prefix (e5) or leave as is\n",
    "        dev_labels.append(int(ex['label']))                 # store gold label\n",
    "\n",
    "    dev_emb1 = encoder(dev_left)                            # encode left dev sentences\n",
    "    dev_emb2 = encoder(dev_right)                           # encode right dev sentences\n",
    "    dev_sims = cosine_sim(dev_emb1, dev_emb2)               # cosine similarities for dev\n",
    "    thr, dev_f1, dev_acc = best_threshold(dev_sims, dev_labels)  # pick best threshold on dev\n",
    "    print(f\"Dev : best_threshold={thr:.4f} | F1={dev_f1:.4f} | Acc={dev_acc:.4f}\")  # report dev calibration\n",
    "\n",
    "    # TEST\n",
    "    test_left, test_right, test_labels = [], [], []         # prepare test containers\n",
    "    for ex in test_data:                                    # iterate test examples\n",
    "        s1 = insert_markers(ex['sentence1'], ex['start1'], ex['end1'])  # mark sentence1\n",
    "        s2 = insert_markers(ex['sentence2'], ex['start2'], ex['end2'])  # mark sentence2\n",
    "        test_left.append(maybe_prefix(model_id, s1))        # prefix if e5\n",
    "        test_right.append(maybe_prefix(model_id, s2))       # prefix if e5\n",
    "        test_labels.append(int(ex['label']))                # gold label\n",
    "\n",
    "    test_emb1 = encoder(test_left)                          # encode left test sentences\n",
    "    test_emb2 = encoder(test_right)                         # encode right test sentences\n",
    "    test_sims = cosine_sim(test_emb1, test_emb2)            # cosine similarities\n",
    "    test_pred = (test_sims >= thr).astype(int)              # predict using calibrated threshold\n",
    "\n",
    "    test_f1  = f1_score(test_labels, test_pred)             # test F1\n",
    "    test_acc = accuracy_score(test_labels, test_pred)       # test Accuracy\n",
    "    print(f\"Test : F1={test_f1:.4f} | Acc={test_acc:.4f}\")  # report test metrics\n",
    "\n",
    "    # Save per-pair predictions\n",
    "    rows = []                                               # per-pair rows to write\n",
    "    for ex, sim, pred in zip(test_data, test_sims, test_pred):  # iterate results\n",
    "        rows.append({\n",
    "            'model': model_name,                            # short model name\n",
    "            'lemma': ex.get('lemma', ''),                   # lemma (if present)\n",
    "            'sent_id1': ex.get('sent_id1', ''),             # sentence id 1\n",
    "            'sent_id2': ex.get('sent_id2', ''),             # sentence id 2\n",
    "            'sim': float(sim),                              # cosine similarity\n",
    "            'pred': int(pred),                              # predicted label\n",
    "            'label': int(ex.get('label', 0)),               # gold label (default 0 if missing)\n",
    "        })\n",
    "    pred_path = os.path.join(out_dir, f'{model_name}_zeroshot_test_predictions.csv')  # CSV path\n",
    "    pd.DataFrame(rows).to_csv(pred_path, index=False, encoding='utf-8-sig')          # write CSV with BOM\n",
    "\n",
    "    return {\n",
    "        'model': model_name,                                # echo model name\n",
    "        'thresh': thr,                                      # chosen threshold\n",
    "        'dev_f1': dev_f1, 'dev_acc': dev_acc,               # dev metrics\n",
    "        'test_f1': test_f1, 'test_acc': test_acc,           # test metrics\n",
    "        'pred_path': pred_path                              # where predictions were saved\n",
    "    }\n",
    "\n",
    "\n",
    "# Function: main\n",
    "# Purpose : Orchestrate the full zero-shot evaluation across configured models.\n",
    "# Inputs  : \n",
    "#   None (uses global config for paths/models).\n",
    "# Outputs : \n",
    "#   None (prints metrics, writes predictions and a summary CSV).\n",
    "def main():\n",
    "    dev_data  = load_wic(WIC_DEV)                           # load dev split\n",
    "    test_data = load_wic(WIC_TEST)                          # load test split\n",
    "\n",
    "    summaries = []                                          # accumulate per-model summaries\n",
    "    t0 = time.time()                                        # start timer\n",
    "    for name, mid in MODELS:                                # iterate configured models\n",
    "        try:\n",
    "            s = eval_model(name, mid, dev_data, test_data, out_dir=OUT_DIR)  # evaluate one model\n",
    "            summaries.append(s)                             # store summary\n",
    "        except Exception as e:                              # robust loop: continue on error\n",
    "            print(f\"[WARN] {name} failed: {e}\")             # report failure\n",
    "\n",
    "    if summaries:                                           # if we collected any results\n",
    "        sum_df = pd.DataFrame(summaries)                    # tabularize summaries\n",
    "        sum_csv = os.path.join(OUT_DIR, 'zeroshot_calibrated_summary.csv')  # summary path\n",
    "        sum_df.to_csv(sum_csv, index=False, encoding='utf-8-sig')           # write summary CSV\n",
    "        print(\"\\nCalibrated zero-shot summary\")       # header\n",
    "        print(sum_df.to_string(index=False))                # pretty-print table\n",
    "        print(f\"\\nSaved summary to: {sum_csv}\")             # location info\n",
    "    print(f\"\\nDone in {time.time()-t0:.1f}s\")               # total elapsed time\n",
    "\n",
    "if __name__ == '__main__':                                  # script entry point\n",
    "    main()                                                  # run main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01676c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers>=4.30 in c:\\users\\student\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (4.52.4)\n",
      "Collecting transformers>=4.30\n",
      "  Downloading transformers-4.55.0-py3-none-any.whl.metadata (39 kB)\n",
      "Collecting accelerate>=0.20\n",
      "  Downloading accelerate-1.10.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\student\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers>=4.30) (3.18.0)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers>=4.30)\n",
      "  Downloading huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\student\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers>=4.30) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\student\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers>=4.30) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\student\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers>=4.30) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\student\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers>=4.30) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\student\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers>=4.30) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\student\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers>=4.30) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\student\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers>=4.30) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\student\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers>=4.30) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\student\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.30) (2025.5.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\student\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.30) (4.14.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\student\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from accelerate>=0.20) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\student\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from accelerate>=0.20) (2.7.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\student\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from torch>=2.0.0->accelerate>=0.20) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\student\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from torch>=2.0.0->accelerate>=0.20) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\student\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from torch>=2.0.0->accelerate>=0.20) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\student\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from torch>=2.0.0->accelerate>=0.20) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\student\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=0.20) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\student\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tqdm>=4.27->transformers>=4.30) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\student\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from jinja2->torch>=2.0.0->accelerate>=0.20) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\student\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests->transformers>=4.30) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\student\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests->transformers>=4.30) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\student\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests->transformers>=4.30) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\student\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests->transformers>=4.30) (2025.4.26)\n",
      "Downloading transformers-4.55.0-py3-none-any.whl (11.3 MB)\n",
      "   ---------------------------------------- 0.0/11.3 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.8/11.3 MB 4.6 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 1.8/11.3 MB 4.6 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.9/11.3 MB 4.6 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 3.7/11.3 MB 4.6 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 4.7/11.3 MB 4.6 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 5.8/11.3 MB 4.6 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 6.6/11.3 MB 4.6 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 7.6/11.3 MB 4.6 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.4/11.3 MB 4.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 9.4/11.3 MB 4.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.5/11.3 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.3/11.3 MB 4.5 MB/s  0:00:02\n",
      "Downloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "   ---------------------------------------- 0.0/561.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 561.5/561.5 kB 3.8 MB/s  0:00:00\n",
      "Downloading accelerate-1.10.0-py3-none-any.whl (374 kB)\n",
      "Installing collected packages: huggingface-hub, accelerate, transformers\n",
      "\n",
      "  Attempting uninstall: huggingface-hub\n",
      "\n",
      "    Found existing installation: huggingface-hub 0.32.4\n",
      "\n",
      "    Uninstalling huggingface-hub-0.32.4:\n",
      "\n",
      "      Successfully uninstalled huggingface-hub-0.32.4\n",
      "\n",
      "   ---------------------------------------- 0/3 [huggingface-hub]\n",
      "   ---------------------------------------- 0/3 [huggingface-hub]\n",
      "   ---------------------------------------- 0/3 [huggingface-hub]\n",
      "   ---------------------------------------- 0/3 [huggingface-hub]\n",
      "   ---------------------------------------- 0/3 [huggingface-hub]\n",
      "   ---------------------------------------- 0/3 [huggingface-hub]\n",
      "   ---------------------------------------- 0/3 [huggingface-hub]\n",
      "   ---------------------------------------- 0/3 [huggingface-hub]\n",
      "   ---------------------------------------- 0/3 [huggingface-hub]\n",
      "   ---------------------------------------- 0/3 [huggingface-hub]\n",
      "   ---------------------------------------- 0/3 [huggingface-hub]\n",
      "   ---------------------------------------- 0/3 [huggingface-hub]\n",
      "   ---------------------------------------- 0/3 [huggingface-hub]\n",
      "   ---------------------------------------- 0/3 [huggingface-hub]\n",
      "   ---------------------------------------- 0/3 [huggingface-hub]\n",
      "   ---------------------------------------- 0/3 [huggingface-hub]\n",
      "   ---------------------------------------- 0/3 [huggingface-hub]\n",
      "   ---------------------------------------- 0/3 [huggingface-hub]\n",
      "   ---------------------------------------- 0/3 [huggingface-hub]\n",
      "   ------------- -------------------------- 1/3 [accelerate]\n",
      "   ------------- -------------------------- 1/3 [accelerate]\n",
      "   ------------- -------------------------- 1/3 [accelerate]\n",
      "   ------------- -------------------------- 1/3 [accelerate]\n",
      "   ------------- -------------------------- 1/3 [accelerate]\n",
      "   ------------- -------------------------- 1/3 [accelerate]\n",
      "   ------------- -------------------------- 1/3 [accelerate]\n",
      "   ------------- -------------------------- 1/3 [accelerate]\n",
      "   ------------- -------------------------- 1/3 [accelerate]\n",
      "   ------------- -------------------------- 1/3 [accelerate]\n",
      "  Attempting uninstall: transformers\n",
      "   ------------- -------------------------- 1/3 [accelerate]\n",
      "    Found existing installation: transformers 4.52.4\n",
      "   ------------- -------------------------- 1/3 [accelerate]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "    Uninstalling transformers-4.52.4:\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "      Successfully uninstalled transformers-4.52.4\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   ---------------------------------------- 3/3 [transformers]\n",
      "\n",
      "Successfully installed accelerate-1.10.0 huggingface-hub-0.34.4 transformers-4.55.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Installs/updates Hugging Face Transformers (>=4.30) and Accelerate (>=0.20)\n",
    "pip install -U \"transformers>=4.30\" \"accelerate>=0.20\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56deec38",
   "metadata": {},
   "source": [
    "**Few-/Full-Shot Fine-Tuning for Bangla WiC with a Trainer-Free PyTorch Loop (on Uncapped WIC dataset)**\n",
    "\n",
    "This script fine-tunes multiple Hugging Face transformer backbones on a Bangla Word-in-Context (WiC) dataset using a trainer-free PyTorch loop. It injects [TGT] ... [/TGT] markers around the target spans, supports few-shot regimes (e.g., 5%, 10%, 20%, 30%) and full-shot (100%), performs early stopping on dev F1, and saves the best checkpoint plus CSV summaries. It runs each (model × regime) combination reproducibly (seeded), evaluates on the dev set per epoch and on the test set at the end, and writes consolidated results and a pivot table under the output directory.\n",
    "\n",
    " 1) Define input/output folders, model IDs, regimes, and training hyperparameters.\n",
    " 2) Select device (cuda if available) and set random seeds for Python/NumPy/Transformers for reproducibility.\n",
    " 3) Read wic_train.json, wic_dev.json, and wic_test.json from DATA_DIR via load_json_array.\n",
    " 4) Convert raw JSON lists to tidy DataFrames with standardized columns using to_dataframe.\n",
    " 5) Wrap target spans with [TGT] and [/TGT] using with_markers.\n",
    " 6) Tokenize paired sentences with a Hugging Face tokenizer in tokenize_pairs (no padding here; a collator pads dynamically).\n",
    " 7) For each regime fraction, draw a stratified subset of the training DataFrame (preserving label balance) using stratified_fraction.\n",
    " 8) Wrap encodings/labels into a lightweight WiCDataset.\n",
    " 9) Build DataLoaders for train/dev/test with DataCollatorWithPadding for efficient dynamic padding.\n",
    " 10) Load AutoModelForSequenceClassification (binary head) and tokenizer per model.\n",
    " 11) Add special tokens ([TGT], [/TGT]) and resize embeddings accordingly.\n",
    " 12) Train with AdamW and a linear warmup + decay schedule (get_linear_schedule_with_warmup).\n",
    " 13) Compute total and warmup steps from loader size and MAX_EPOCHS.\n",
    " 14) For each epoch:\n",
    "    - Forward pass with labels → compute cross-entropy loss.\n",
    "    - Backprop, gradient clipping, optimizer and scheduler steps.\n",
    "    - Evaluate on the dev set using epoch_eval to get accuracy and F1. \n",
    " 15) Track best dev F1; when improved, save model + tokenizer and a small JSON with dev metrics.\n",
    " 16) Stop early after PATIENCE epochs without dev-F1 improvement.\n",
    " 17) Reload the best checkpoint (if available) and evaluate on the test set (accuracy, F1)\n",
    " 18) Save per-run train_history.csv and summary.csv in a run-specific folder.\n",
    " 19) Aggregate all runs into ALL_results.csv and a sorted pivot ALL_results_pivot.csv showing test metrics by model and regime.\n",
    " 20) Print the final pivot table to the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5efd3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, random, time                               # Std: filesystem, JSON I/O, RNG seeding, timing\n",
    "from dataclasses import dataclass                           # For lightweight dataset container\n",
    "from typing import Dict, List                               # Type hints for clarity\n",
    "import numpy as np                                          # Numerical ops for logits → metrics\n",
    "import pandas as pd                                         # Tabular wrangling of WiC lists\n",
    "import torch                                                # Core PyTorch\n",
    "from torch.utils.data import DataLoader                     # Mini-batching utilities\n",
    "from sklearn.metrics import accuracy_score, f1_score        # Standard classification metrics\n",
    "\n",
    "import transformers                                         # HF Transformers library (version printed below)\n",
    "from transformers import (                                  # Selected utilities/classes from Transformers\n",
    "    AutoTokenizer, AutoConfig, AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding, get_linear_schedule_with_warmup, set_seed\n",
    ")\n",
    "\n",
    "print(\"[info] transformers version:\", transformers.__version__)  # Log Transformers version for reproducibility\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "DATA_DIR = r\"C:\\Users\\Student\\Downloads\\projecting_sentences-main\\projecting_sentences-main\\Dataset\\bangla_wic_dataset_uncapped\"  # Root folder containing wic_train/dev/test.json\n",
    "OUT_DIR  = r\"C:\\Users\\Student\\Downloads\\projecting_sentences-main\\projecting_sentences-main\\Result\\WIC_finetuned_result\"   # Output root for checkpoints & CSVs\n",
    "\n",
    "MODELS = {     # HF model id per short name to iterate over\n",
    "    \"sahajbert\": \"neuropark/sahajBERT\",\n",
    "    \"muril\"    : \"google/muril-base-cased\",\n",
    "    \"labse\"    : \"sentence-transformers/LaBSE\",\n",
    "    \"e5\"       : \"intfloat/multilingual-e5-base\",\n",
    "    \"banglabert\":\"sagorsarker/bangla-bert-base\",\n",
    "    # Note: all are fine-tuned as sequence classifiers\n",
    "}\n",
    "\n",
    "REGIMES = [0.05, 0.10, 0.20, 0.30, 1.00]                     # Few-shot fractions and full-shot (100%)\n",
    "\n",
    "SEED = 42                                                    # Global reproducibility seed\n",
    "BATCH_SIZE = 16                                              # Per-step batch size\n",
    "MAX_EPOCHS = 5                                               # Maximum epochs before stopping\n",
    "LR = 2e-5                                                    # AdamW learning rate\n",
    "WARMUP_RATIO = 0.06                                          # Linear warmup proportion of total steps\n",
    "PATIENCE = 2                                                 # Early-stopping: epochs without dev-F1 improvement\n",
    "MAX_LEN = 256                                                # Max sequence length for tokenization\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Prefer GPU if available\n",
    "\n",
    "\n",
    "# IO & UTILS\n",
    "\n",
    "# Function: load_json_array\n",
    "# Purpose: Read a WiC split from disk (JSON list of dicts) and return it as a Python list.\n",
    "# Inputs:\n",
    "#   path (str): filesystem path to a JSON file containing the WiC examples.\n",
    "# Outputs:\n",
    "#   List[Dict]: a Python list where each element is a dictionary for one WiC example.\n",
    "def load_json_array(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:            # Open the JSON file in UTF-8 mode\n",
    "        return json.load(f)                                  # Parse and return as Python list\n",
    "\n",
    "\n",
    "# Function: with_markers\n",
    "# Purpose: Insert explicit [TGT] and [/TGT] markers into a sentence around a target span.\n",
    "# Inputs:\n",
    "#   sentence (str): the full sentence text.\n",
    "#   start (int): character start index (inclusive) of the target span.\n",
    "#   end (int): character end index (exclusive) of the target span.\n",
    "#   open_tok (str): left marker token (default \"[TGT]\").\n",
    "#   close_tok (str): right marker token (default \"[/TGT]\").\n",
    "# Outputs:\n",
    "#   str: the sentence with markers injected around the specified span (clamped to safe range).\n",
    "def with_markers(sentence: str, start: int, end: int, open_tok=\"[TGT]\", close_tok=\"[/TGT]\"):\n",
    "    start = max(0, int(start)); end = max(start, int(end))   # Clamp indices and enforce start ≤ end\n",
    "    return sentence[:start] + open_tok + sentence[start:end] + close_tok + sentence[end:]  # Return marked sentence\n",
    "\n",
    "\n",
    "# Function: to_dataframe\n",
    "# Purpose: Convert a raw WiC JSON list into a tidy DataFrame with standardized columns for training.\n",
    "# Inputs:\n",
    "#   wic_list (List[Dict]): list of WiC examples read from JSON.\n",
    "# Outputs:\n",
    "#   pd.DataFrame: columns = lemma, s1, s2, sid1, sid2, start1, end1, start2, end2, label (int).\n",
    "def to_dataframe(wic_list: List[Dict]):\n",
    "    rows = []                                                # Accumulator for row dicts\n",
    "    for x in wic_list:                                       # Iterate over raw examples\n",
    "        rows.append({\n",
    "            \"lemma\": x[\"lemma\"],                             # Lemma string\n",
    "            \"s1\": x[\"sentence1\"],                            # Left sentence\n",
    "            \"s2\": x[\"sentence2\"],                            # Right sentence\n",
    "            \"sid1\": x[\"sent_id1\"],                           # Left sentence id\n",
    "            \"sid2\": x[\"sent_id2\"],                           # Right sentence id\n",
    "            \"start1\": int(x[\"start1\"]), \"end1\": int(x[\"end1\"]),  # Left target span (char offsets)\n",
    "            \"start2\": int(x[\"start2\"]), \"end2\": int(x[\"end2\"]),  # Right target span (char offsets)\n",
    "            \"label\": int(x[\"label\"]),                        # Gold label (1 = same sense, 0 = different)\n",
    "        })\n",
    "    return pd.DataFrame(rows)                                # Build DataFrame from rows\n",
    "\n",
    "\n",
    "# Function: stratified_fraction\n",
    "# Purpose: Take a stratified sample of the DataFrame by label to achieve a given fraction for few-shot regimes.\n",
    "# Inputs:\n",
    "#   df (pd.DataFrame): full training DataFrame with a 'label' column.\n",
    "#   frac (float): fraction to sample; if ≥ 1.0, return a full shuffle.\n",
    "#   seed (int): RNG seed for reproducibility.\n",
    "# Outputs:\n",
    "#   pd.DataFrame: sampled DataFrame with near-constant label proportions.\n",
    "def stratified_fraction(df: pd.DataFrame, frac: float, seed: int):\n",
    "    if frac >= 1.0:                                          # Full-shot case\n",
    "        return df.sample(frac=1.0, random_state=seed).reset_index(drop=True)  # Return shuffled copy\n",
    "    parts = []                                               # Accumulate per-class samples\n",
    "    for y, sub in df.groupby(\"label\"):                       # Split by class label\n",
    "        k = max(1, int(round(frac * len(sub))))              # Class-size specific sample count\n",
    "        parts.append(sub.sample(n=k, random_state=seed))     # Sample that many rows from this class\n",
    "    return pd.concat(parts, axis=0).sample(frac=1.0, random_state=seed).reset_index(drop=True)  # Recombine & shuffle\n",
    "\n",
    "\n",
    "# Dataclass: WiCDataset\n",
    "# Purpose: Minimal Dataset wrapper holding token encodings and labels for the Trainer-free loop.\n",
    "# Fields:\n",
    "#   encodings (Dict[str, List[List[int]]]): tokenized features (input_ids, attention_mask, etc.) as lists.\n",
    "#   labels (List[int]): gold labels aligned to encodings.\n",
    "@dataclass\n",
    "class WiCDataset(torch.utils.data.Dataset):\n",
    "    encodings: Dict[str, List[List[int]]]                    # Token features dictionary\n",
    "    labels: List[int]                                        # Gold labels per example\n",
    "    def __len__(self): return len(self.labels)               # Return dataset size\n",
    "    def __getitem__(self, idx):                              # Retrieve one item by index\n",
    "        # return python lists; collator will pad & convert to tensors\n",
    "        item = {k: v[idx] for k, v in self.encodings.items()}  # Slice each feature list\n",
    "        item[\"labels\"] = self.labels[idx]                    # Attach the corresponding label\n",
    "        return item                                          # Return a dict expected by the model\n",
    "\n",
    "\n",
    "# Function: tokenize_pairs\n",
    "# Purpose: Insert markers into sentence pairs and tokenize them with a given tokenizer.\n",
    "# Inputs:\n",
    "#   df (pd.DataFrame): must include s1, s2, start1, end1, start2, end2, and label.\n",
    "#   tokenizer: a Hugging Face tokenizer instance.\n",
    "# Outputs:\n",
    "#   (encodings, labels): encodings is a dict of token lists; labels is a list[int].\n",
    "def tokenize_pairs(df: pd.DataFrame, tokenizer):\n",
    "    OPEN, CLOSE = \"[TGT]\", \"[/TGT]\"                          # Consistent marker tokens\n",
    "    s1 = [with_markers(a, s, e, OPEN, CLOSE) for a, s, e in zip(df[\"s1\"], df[\"start1\"], df[\"end1\"])]  # Left marked\n",
    "    s2 = [with_markers(b, s, e, OPEN, CLOSE) for b, s, e in zip(df[\"s2\"], df[\"start2\"], df[\"end2\"])]  # Right marked\n",
    "    enc = tokenizer(                                         # Tokenize sentence pairs\n",
    "        s1, s2,\n",
    "        truncation=True,                                     # Truncate to MAX_LEN\n",
    "        max_length=MAX_LEN,\n",
    "        padding=False                                        # Dynamic padding handled by collator\n",
    "    )\n",
    "    labels = df[\"label\"].astype(int).tolist()                # Extract labels as ints\n",
    "    return enc, labels                                       # Return features and labels\n",
    "\n",
    "\n",
    "# Function: compute_metrics_from_logits\n",
    "# Purpose: Convert raw model logits to predictions and compute accuracy and F1.\n",
    "# Inputs:\n",
    "#   logits (np.ndarray): shape [N, 2] for binary classification.\n",
    "#   labels (List[int]): gold labels (0/1) of length N.\n",
    "# Outputs:\n",
    "#   (acc, f1): tuple of floats with accuracy and F1 score.\n",
    "def compute_metrics_from_logits(logits: np.ndarray, labels: List[int]):\n",
    "    preds = logits.argmax(axis=-1)                           # Predicted class = argmax over logits\n",
    "    acc = accuracy_score(labels, preds)                      # Compute accuracy\n",
    "    f1  = f1_score(labels, preds)                            # Compute F1 (binary average='binary')\n",
    "    return acc, f1                                           # Return both metrics\n",
    "\n",
    "\n",
    "# TRAIN / EVAL\n",
    "\n",
    "# Function: epoch_eval\n",
    "# Purpose: Evaluate a model over a dataloader and compute accuracy and F1 from accumulated logits.\n",
    "# Inputs:\n",
    "#   model (nn.Module): sequence classification model.\n",
    "#   dataloader (DataLoader): batched dataset iterator.\n",
    "#   device (torch.device): 'cuda' or 'cpu' to run inference on.\n",
    "# Outputs:\n",
    "#   (acc, f1): floats with accuracy and F1 on the provided dataloader.\n",
    "def epoch_eval(model, dataloader, device):\n",
    "    model.eval()                                             # Switch to eval mode (no dropout, etc.)\n",
    "    all_logits = []                                          # Accumulator for logits\n",
    "    all_labels = []                                          # Accumulator for gold labels\n",
    "    with torch.no_grad():                                    # Disable gradient tracking\n",
    "        for batch in dataloader:                             # Iterate over batches\n",
    "            labels = batch.pop(\"labels\").to(device)          # Move labels to device and remove from features\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}  # Move all features to device\n",
    "            outputs = model(**batch)                         # Forward pass\n",
    "            logits = outputs.logits.detach().cpu().numpy()   # Collect logits on CPU as NumPy\n",
    "            all_logits.append(logits)                        # Append batch logits\n",
    "            all_labels.append(labels.cpu().numpy())          # Append batch labels\n",
    "    all_logits = np.concatenate(all_logits, axis=0) if all_logits else np.zeros((0,2))  # Stack logits\n",
    "    all_labels = np.concatenate(all_labels, axis=0).astype(int) if all_labels else np.zeros((0,), dtype=int)  # Stack labels\n",
    "    return compute_metrics_from_logits(all_logits, all_labels)  # Compute and return metrics\n",
    "\n",
    "\n",
    "# Function: train_eval_one\n",
    "# Purpose: Fine-tune one backbone at a given regime, with early stopping on dev F1; save best; evaluate on test.\n",
    "# Inputs:\n",
    "#   model_id (str): Hugging Face model identifier.\n",
    "#   model_name (str): short alias for naming outputs.\n",
    "#   df_train, df_dev, df_test (pd.DataFrame): split dataframes from WiC JSONs.\n",
    "#   regime_frac (float): fraction of the training set to sample (few-/full-shot).\n",
    "#   out_root (str): output directory root for this model/regime run.\n",
    "# Outputs:\n",
    "#   Dict: a single summary row containing sizes, best dev F1, and test metrics.\n",
    "def train_eval_one(model_id, model_name, df_train, df_dev, df_test, regime_frac, out_root):\n",
    "    set_seed(SEED)                                           # Set HF/torch/random seeds\n",
    "\n",
    "    # Tokenizer & model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)  # Load tokenizer\n",
    "    tokenizer.add_special_tokens({\"additional_special_tokens\": [\"[TGT]\", \"[/TGT]\"]})  # Add markers to vocab\n",
    "\n",
    "    config = AutoConfig.from_pretrained(model_id, num_labels=2)  # Binary classification head\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_id, config=config)  # Load base weights\n",
    "    model.resize_token_embeddings(len(tokenizer))            # Resize embeddings due to added tokens\n",
    "    model.to(DEVICE)                                        # Move model to GPU/CPU\n",
    "\n",
    "    # Few-shot sample\n",
    "    df_subtrain = stratified_fraction(df_train, regime_frac, seed=SEED)  # Stratified downsample of train\n",
    "\n",
    "    # Tokenize\n",
    "    enc_tr, y_tr = tokenize_pairs(df_subtrain, tokenizer)    # Tokenize sampled train\n",
    "    enc_dv, y_dv = tokenize_pairs(df_dev, tokenizer)         # Tokenize dev\n",
    "    enc_ts, y_ts = tokenize_pairs(df_test, tokenizer)        # Tokenize test\n",
    "\n",
    "    # Datasets / Loaders\n",
    "    collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8 if torch.cuda.is_available() else None)  # Dynamic pad\n",
    "    ds_tr = WiCDataset(enc_tr, y_tr)                         # Train dataset\n",
    "    ds_dv = WiCDataset(enc_dv, y_dv)                         # Dev dataset\n",
    "    ds_ts = WiCDataset(enc_ts, y_ts)                         # Test dataset\n",
    "\n",
    "    train_loader = DataLoader(ds_tr, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collator, num_workers=0)   # Train loader\n",
    "    dev_loader   = DataLoader(ds_dv, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collator, num_workers=0)  # Dev loader\n",
    "    test_loader  = DataLoader(ds_ts, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collator, num_workers=0)  # Test loader\n",
    "\n",
    "    # Optim / Scheduler\n",
    "    total_steps = max(1, len(train_loader) * MAX_EPOCHS)     # Total train steps for scheduler\n",
    "    warmup_steps = int(WARMUP_RATIO * total_steps)           # Num warmup steps\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR) # AdamW optimizer\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)  # Linear warmup/decay\n",
    "\n",
    "    # Output dir\n",
    "    tag = f\"{model_name}_frac{int(regime_frac*100)}\"         # Tag includes model alias and regime\n",
    "    out_dir = os.path.join(out_root, tag); os.makedirs(out_dir, exist_ok=True)  # Make output folder\n",
    "\n",
    "    best_dev_f1 = -1.0                                       # Track best dev F1 for early stopping\n",
    "    no_improve = 0                                           # Counter for patience\n",
    "    history = []                                             # Per-epoch logs\n",
    "\n",
    "    print(f\"\\n[run] {model_name} | frac={regime_frac} | train={len(ds_tr)} dev={len(ds_dv)} test={len(ds_ts)}\")  # Run header\n",
    "\n",
    "    for epoch in range(1, MAX_EPOCHS+1):                     # Epoch loop\n",
    "        model.train()                                        # Train mode\n",
    "        t0 = time.time()                                     # Epoch timer\n",
    "        total_loss = 0.0                                     # Reset loss accumulator\n",
    "        for batch in train_loader:                           # Iterate training batches\n",
    "            labels = batch.pop(\"labels\").to(DEVICE)          # Extract and move labels\n",
    "            batch  = {k: v.to(DEVICE) for k, v in batch.items()}  # Move features to device\n",
    "\n",
    "            outputs = model(**batch, labels=labels)          # Forward pass with labels\n",
    "            loss = outputs.loss                              # Cross-entropy loss\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)            # Clear grads\n",
    "            loss.backward()                                  # Backprop\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping for stability\n",
    "            optimizer.step()                                 # Optimizer step\n",
    "            scheduler.step()                                 # Scheduler step\n",
    "\n",
    "            total_loss += loss.item()                        # Accumulate batch loss\n",
    "\n",
    "        # Eval on dev each epoch\n",
    "        dev_acc, dev_f1 = epoch_eval(model, dev_loader, DEVICE)  # Validate on dev\n",
    "        avg_loss = total_loss / max(1, len(train_loader))    # Compute mean train loss for epoch\n",
    "        history.append({\"epoch\": epoch, \"train_loss\": avg_loss, \"dev_acc\": dev_acc, \"dev_f1\": dev_f1})  # Log row\n",
    "        print(f\"  epoch {epoch:02d} | loss {avg_loss:.4f} | dev_acc {dev_acc:.4f} | dev_f1 {dev_f1:.4f} | {time.time()-t0:.1f}s\")  # Epoch summary\n",
    "\n",
    "        # Early stopping on dev F1\n",
    "        if dev_f1 > best_dev_f1:                             # If improved dev F1\n",
    "            best_dev_f1 = dev_f1                             # Update best\n",
    "            no_improve = 0                                   # Reset patience counter\n",
    "            # Save best\n",
    "            model.save_pretrained(out_dir)                   # Persist model weights/config\n",
    "            tokenizer.save_pretrained(out_dir)               # Persist tokenizer artifacts\n",
    "            with open(os.path.join(out_dir, \"dev_best.json\"), \"w\", encoding=\"utf-8\") as f:  # Save best dev metrics\n",
    "                json.dump({\"epoch\": epoch, \"dev_acc\": dev_acc, \"dev_f1\": dev_f1}, f, ensure_ascii=False, indent=2)\n",
    "        else:\n",
    "            no_improve += 1                                  # No improvement this epoch\n",
    "            if no_improve >= PATIENCE:                       # Hit patience threshold\n",
    "                print(f\"  early stop: no dev F1 improvement for {PATIENCE} epoch(s).\")  # Log early stop\n",
    "                break                                        # Exit training loop\n",
    "\n",
    "    # Load best (already saved) and evaluate on test\n",
    "    # (Re-use in-memory 'model' which holds last epoch; for exact best, reload)\n",
    "    try:\n",
    "        best_model = AutoModelForSequenceClassification.from_pretrained(out_dir).to(DEVICE)  # Reload best if present\n",
    "    except Exception:\n",
    "        best_model = model                                  # Otherwise use last-epoch model\n",
    "    test_acc, test_f1 = epoch_eval(best_model, test_loader, DEVICE)  # Final test evaluation\n",
    "\n",
    "    # Save summary row\n",
    "    row = {\n",
    "        \"model\": model_name, \"hf_id\": model_id, \"regime_frac\": regime_frac,  # Identity fields\n",
    "        \"train_size\": len(ds_tr), \"dev_size\": len(ds_dv), \"test_size\": len(ds_ts),  # Dataset sizes\n",
    "        \"best_dev_f1\": float(best_dev_f1), \"test_acc\": float(test_acc), \"test_f1\": float(test_f1)  # Metrics\n",
    "    }\n",
    "    pd.DataFrame(history).to_csv(os.path.join(out_dir, \"train_history.csv\"), index=False, encoding=\"utf-8-sig\")  # Persist per-epoch log\n",
    "    pd.DataFrame([row]).to_csv(os.path.join(out_dir, \"summary.csv\"), index=False, encoding=\"utf-8-sig\")          # Persist run summary\n",
    "    return row                                               # Return summary dict to caller\n",
    "\n",
    "\n",
    "# MAIN \n",
    "\n",
    "# Function: main\n",
    "# Purpose: End-to-end orchestration—load splits, prepare DataFrames, and iterate model × regime runs; collect summaries.\n",
    "# Inputs: \n",
    "#   None (uses module-level config paths and constants).\n",
    "# Outputs: \n",
    "#   None (Files written under OUT_DIR; prints run tables).\n",
    "def main():\n",
    "    random.seed(SEED); np.random.seed(SEED); set_seed(SEED)  # Seed Python, NumPy, and Transformers\n",
    "\n",
    "    train = load_json_array(os.path.join(DATA_DIR, \"wic_train.json\"))  # Read train split JSON\n",
    "    dev   = load_json_array(os.path.join(DATA_DIR, \"wic_dev.json\"))    # Read dev split JSON\n",
    "    test  = load_json_array(os.path.join(DATA_DIR, \"wic_test.json\"))   # Read test split JSON\n",
    "\n",
    "    df_train = to_dataframe(train)                          # Convert train JSON → DataFrame\n",
    "    df_dev   = to_dataframe(dev)                            # Convert dev JSON → DataFrame\n",
    "    df_test  = to_dataframe(test)                           # Convert test JSON → DataFrame\n",
    "\n",
    "    runs_dir = os.path.join(OUT_DIR, \"runs\"); os.makedirs(runs_dir, exist_ok=True)  # Ensure runs folder exists\n",
    "    results = []                                            # Accumulate per-run summaries\n",
    "    for model_name, model_id in MODELS.items():             # Iterate models\n",
    "        for frac in REGIMES:                                # Iterate data regimes\n",
    "            try:\n",
    "                res = train_eval_one(model_id, model_name, df_train, df_dev, df_test, frac, runs_dir)  # Run a job\n",
    "                results.append(res)                         # Keep summary row\n",
    "            except Exception as e:                          # Robustness: catch & log errors per run\n",
    "                print(f\"[WARN] Run failed for {model_name} (frac={frac}): {e}\")\n",
    "\n",
    "    if results:                                             # If at least one successful run\n",
    "        df_res = pd.DataFrame(results)                      # Build a table of summaries\n",
    "        df_res.to_csv(os.path.join(OUT_DIR, \"ALL_results.csv\"), index=False, encoding=\"utf-8-sig\")  # Save summaries CSV\n",
    "        piv = df_res.pivot_table(index=[\"model\",\"regime_frac\"], values=[\"test_acc\",\"test_f1\"], aggfunc=\"mean\")  # Pivot by model/regime\n",
    "        piv = piv.reset_index().sort_values([\"model\",\"regime_frac\"])  # Sort for readability\n",
    "        piv.to_csv(os.path.join(OUT_DIR, \"ALL_results_pivot.csv\"), index=False, encoding=\"utf-8-sig\")  # Save pivot CSV\n",
    "        print(\"\\n== Final (test) results ==\\n\", piv)        # Print final pivot table\n",
    "    else:\n",
    "        print(\"No successful runs to summarize.\")           # Inform if nothing completed\n",
    "\n",
    "if __name__ == \"__main__\":                                  # Script entry point\n",
    "    main()                                                  # Launch the pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa65b28",
   "metadata": {},
   "source": [
    "Code cell output from Google colab:\n",
    "\n",
    "[info] transformers version: 4.55.1\n",
    "\n",
    "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at neuropark/sahajBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
    "\n",
    "[run] sahajbert | frac=0.05 | train=5186 dev=4714 test=4270\n",
    "-  epoch 01 | loss 0.6360 | dev_acc 0.7567 | dev_f1 0.7624 | 133.6s\n",
    "-  epoch 02 | loss 0.4427 | dev_acc 0.7804 | dev_f1 0.7946 | 134.0s\n",
    "-  epoch 03 | loss 0.2491 | dev_acc 0.7885 | dev_f1 0.7977 | 133.8s\n",
    "-  epoch 04 | loss 0.1333 | dev_acc 0.8046 | dev_f1 0.8066 | 132.9s\n",
    "-  epoch 05 | loss 0.0686 | dev_acc 0.8089 | dev_f1 0.8086 | 131.7s\n",
    "\n",
    "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at neuropark/sahajBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
    "\n",
    "[run] sahajbert | frac=0.1 | train=10372 dev=4714 test=4270\n",
    "-  epoch 01 | loss 0.5921 | dev_acc 0.7683 | dev_f1 0.7762 | 234.0s\n",
    "-  epoch 02 | loss 0.2871 | dev_acc 0.8318 | dev_f1 0.8361 | 231.7s\n",
    "-  epoch 03 | loss 0.1438 | dev_acc 0.8437 | dev_f1 0.8464 | 232.7s\n",
    "-  epoch 04 | loss 0.0582 | dev_acc 0.8498 | dev_f1 0.8526 | 234.2s\n",
    "-  epoch 05 | loss 0.0194 | dev_acc 0.8477 | dev_f1 0.8510 | 233.5s\n",
    "\n",
    "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at neuropark/sahajBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
    "\n",
    "[run] sahajbert | frac=0.2 | train=20742 dev=4714 test=4270\n",
    "-  epoch 01 | loss 0.4881 | dev_acc 0.8210 | dev_f1 0.8281 | 434.5s\n",
    "-  epoch 02 | loss 0.1722 | dev_acc 0.8471 | dev_f1 0.8483 | 434.9s\n",
    "-  epoch 03 | loss 0.0590 | dev_acc 0.8581 | dev_f1 0.8597 | 437.0s\n",
    "-  epoch 04 | loss 0.0122 | dev_acc 0.8572 | dev_f1 0.8595 | 434.0s\n",
    "-  epoch 05 | loss 0.0033 | dev_acc 0.8579 | dev_f1 0.8625 | 435.4s\n",
    "\n",
    "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at neuropark/sahajBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
    "\n",
    "[run] sahajbert | frac=0.3 | train=31114 dev=4714 test=4270\n",
    "-  epoch 01 | loss 0.3978 | dev_acc 0.8475 | dev_f1 0.8531 | 635.6s\n",
    "-  epoch 02 | loss 0.0974 | dev_acc 0.8729 | dev_f1 0.8768 | 637.6s\n",
    "-  epoch 03 | loss 0.0202 | dev_acc 0.8649 | dev_f1 0.8722 | 635.0s\n",
    "-  epoch 04 | loss 0.0077 | dev_acc 0.8782 | dev_f1 0.8816 | 636.5s\n",
    "-  epoch 05 | loss 0.0011 | dev_acc 0.8793 | dev_f1 0.8839 | 632.9s\n",
    "\n",
    "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at neuropark/sahajBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
    "\n",
    "[run] sahajbert | frac=1.0 | train=103714 dev=4714 test=4270\n",
    "-  epoch 01 | loss 0.2277 | dev_acc 0.8681 | dev_f1 0.8764 | 2033.9s\n",
    "-  epoch 02 | loss 0.0230 | dev_acc 0.8608 | dev_f1 0.8655 | 2029.9s\n",
    "-  epoch 03 | loss 0.0094 | dev_acc 0.8808 | dev_f1 0.8855 | 2029.7s\n",
    "-  epoch 04 | loss 0.0029 | dev_acc 0.8840 | dev_f1 0.8883 | 2036.9s\n",
    "-  epoch 05 | loss 0.0000 | dev_acc 0.8863 | dev_f1 0.8894 | 2037.5s\n",
    "\n",
    "tokenizer_config.json: 100%\n",
    " 206/206 [00:00<00:00, 25.5kB/s]\n",
    "\n",
    "config.json: 100%\n",
    " 411/411 [00:00<00:00, 48.4kB/s]\n",
    "\n",
    "vocab.txt: \n",
    " 3.16M/? [00:00<00:00, 81.7MB/s]\n",
    "\n",
    "special_tokens_map.json: 100%\n",
    " 113/113 [00:00<00:00, 14.6kB/s]\n",
    "\n",
    "pytorch_model.bin: 100%\n",
    " 953M/953M [00:06<00:00, 243MB/s]\n",
    "\n",
    "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/muril-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
    "\n",
    "model.safetensors: 100%\n",
    " 953M/953M [00:05<00:00, 486MB/s]\n",
    "\n",
    "[run] muril | frac=0.05 | train=5186 dev=4714 test=4270\n",
    "-  epoch 01 | loss 0.6872 | dev_acc 0.6860 | dev_f1 0.6043 | 40.5s\n",
    "-  epoch 02 | loss 0.5551 | dev_acc 0.7811 | dev_f1 0.7625 | 40.8s\n",
    "-  epoch 03 | loss 0.3982 | dev_acc 0.8065 | dev_f1 0.8080 | 40.3s\n",
    "-  epoch 04 | loss 0.2852 | dev_acc 0.8004 | dev_f1 0.8092 | 40.6s\n",
    "-  epoch 05 | loss 0.2208 | dev_acc 0.8125 | dev_f1 0.8131 | 40.7s\n",
    "\n",
    "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/muril-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
    "\n",
    "[run] muril | frac=0.1 | train=10372 dev=4714 test=4270\n",
    "-  epoch 01 | loss 0.6618 | dev_acc 0.7359 | dev_f1 0.7354 | 72.0s\n",
    "-  epoch 02 | loss 0.4744 | dev_acc 0.8010 | dev_f1 0.7971 | 72.2s\n",
    "-  epoch 03 | loss 0.2978 | dev_acc 0.8254 | dev_f1 0.8217 | 72.2s\n",
    "-  epoch 04 | loss 0.2062 | dev_acc 0.8422 | dev_f1 0.8448 | 72.5s\n",
    "-  epoch 05 | loss 0.1516 | dev_acc 0.8403 | dev_f1 0.8402 | 72.4s\n",
    "\n",
    "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/muril-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
    "\n",
    "[run] muril | frac=0.2 | train=20742 dev=4714 test=4270\n",
    "-  epoch 01 | loss 0.5644 | dev_acc 0.7951 | dev_f1 0.8168 | 136.7s\n",
    "-  epoch 02 | loss 0.2661 | dev_acc 0.8564 | dev_f1 0.8579 | 136.0s\n",
    "-  epoch 03 | loss 0.1479 | dev_acc 0.8570 | dev_f1 0.8598 | 136.8s\n",
    "-  epoch 04 | loss 0.0813 | dev_acc 0.8619 | dev_f1 0.8633 | 135.5s\n",
    "-  epoch 05 | loss 0.0476 | dev_acc 0.8647 | dev_f1 0.8665 | 136.0s\n",
    "\n",
    "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/muril-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
    "\n",
    "[run] muril | frac=0.3 | train=31114 dev=4714 test=4270\n",
    "-  epoch 01 | loss 0.4953 | dev_acc 0.8377 | dev_f1 0.8300 | 200.9s\n",
    "-  epoch 02 | loss 0.1802 | dev_acc 0.8691 | dev_f1 0.8666 | 199.8s\n",
    "-  epoch 03 | loss 0.0691 | dev_acc 0.8664 | dev_f1 0.8689 | 199.9s\n",
    "-  epoch 04 | loss 0.0290 | dev_acc 0.8721 | dev_f1 0.8721 | 199.0s\n",
    "-  epoch 05 | loss 0.0132 | dev_acc 0.8746 | dev_f1 0.8733 | 199.3s\n",
    "\n",
    "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/muril-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
    "\n",
    "[run] muril | frac=1.0 | train=103714 dev=4714 test=4270\n",
    "-  epoch 01 | loss 0.2852 | dev_acc 0.8729 | dev_f1 0.8709 | 641.1s\n",
    "-  epoch 02 | loss 0.0216 | dev_acc 0.8602 | dev_f1 0.8553 | 641.9s\n",
    "-  epoch 03 | loss 0.0069 | dev_acc 0.8753 | dev_f1 0.8773 | 641.1s\n",
    "-  epoch 04 | loss 0.0027 | dev_acc 0.8801 | dev_f1 0.8809 | 642.4s\n",
    "-  epoch 05 | loss 0.0006 | dev_acc 0.8829 | dev_f1 0.8849 | 641.6s\n",
    "\n",
    "tokenizer_config.json: 100%\n",
    " 397/397 [00:00<00:00, 45.9kB/s]\n",
    "\n",
    "config.json: 100%\n",
    " 804/804 [00:00<00:00, 107kB/s]\n",
    "\n",
    "vocab.txt: \n",
    " 5.22M/? [00:00<00:00, 68.3MB/s]\n",
    "\n",
    "tokenizer.json: \n",
    " 9.62M/? [00:00<00:00, 93.1MB/s]\n",
    "\n",
    "special_tokens_map.json: 100%\n",
    " 112/112 [00:00<00:00, 14.7kB/s]\n",
    "\n",
    "model.safetensors: 100%\n",
    " 1.88G/1.88G [00:08<00:00, 288MB/s]\n",
    "\n",
    "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/LaBSE and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
    "\n",
    "[run] labse | frac=0.05 | train=5186 dev=4714 test=4270\n",
    "-  epoch 01 | loss 0.6548 | dev_acc 0.7183 | dev_f1 0.7002 | 49.9s\n",
    "-  epoch 02 | loss 0.4340 | dev_acc 0.7862 | dev_f1 0.7784 | 49.8s\n",
    "-  epoch 03 | loss 0.2442 | dev_acc 0.7991 | dev_f1 0.7975 | 50.2s\n",
    "-  epoch 04 | loss 0.1357 | dev_acc 0.8021 | dev_f1 0.7988 | 50.2s\n",
    "-  epoch 05 | loss 0.0691 | dev_acc 0.8112 | dev_f1 0.8103 | 49.7s\n",
    "\n",
    "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/LaBSE and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
    "\n",
    "[run] labse | frac=0.1 | train=10372 dev=4714 test=4270  epoch 01 | loss 0.5804 | dev_acc 0.8023 | dev_f1 0.8131 | 89.8s\n",
    "-  epoch 01 | loss 0.5804 | dev_acc 0.8023 | dev_f1 0.8131 | 89.8s\n",
    "-  epoch 02 | loss 0.2603 | dev_acc 0.8426 | dev_f1 0.8437 | 90.0s\n",
    "-  epoch 03 | loss 0.1340 | dev_acc 0.8551 | dev_f1 0.8587 | 90.4s\n",
    "-  epoch 04 | loss 0.0724 | dev_acc 0.8555 | dev_f1 0.8504 | 89.9s\n",
    "-  epoch 05 | loss 0.0323 | dev_acc 0.8583 | dev_f1 0.8585 | 89.4s\n",
    "-  early stop: no dev F1 improvement for 2 epoch(s).\n",
    "\n",
    "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/LaBSE and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
    "\n",
    "[run] labse | frac=0.2 | train=20742 dev=4714 test=4270\n",
    "-  epoch 01 | loss 0.4618 | dev_acc 0.8424 | dev_f1 0.8365 | 171.0s\n",
    "-  epoch 02 | loss 0.1545 | dev_acc 0.8606 | dev_f1 0.8612 | 170.7s\n",
    "-  epoch 03 | loss 0.0608 | dev_acc 0.8727 | dev_f1 0.8705 | 170.7s\n",
    "-  epoch 04 | loss 0.0242 | dev_acc 0.8717 | dev_f1 0.8724 | 170.7s\n",
    "-  epoch 05 | loss 0.0079 | dev_acc 0.8731 | dev_f1 0.8749 | 171.1s\n",
    "\n",
    "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/LaBSE and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
    "\n",
    "[run] labse | frac=0.3 | train=31114 dev=4714 test=4270\n",
    "-  epoch 01 | loss 0.3962 | dev_acc 0.8511 | dev_f1 0.8598 | 251.3s\n",
    "-  epoch 02 | loss 0.1028 | dev_acc 0.8740 | dev_f1 0.8748 | 251.4s\n",
    "-  epoch 03 | loss 0.0293 | dev_acc 0.8757 | dev_f1 0.8774 | 249.8s\n",
    "-  epoch 04 | loss 0.0129 | dev_acc 0.8689 | dev_f1 0.8710 | 250.3s\n",
    "-  epoch 05 | loss 0.0051 | dev_acc 0.8731 | dev_f1 0.8744 | 251.6s\n",
    "-  early stop: no dev F1 improvement for 2 epoch(s).\n",
    "\n",
    "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/LaBSE and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
    "\n",
    "[run] labse | frac=1.0 | train=103714 dev=4714 test=4270\n",
    "-  epoch 01 | loss 0.2199 | dev_acc 0.9005 | dev_f1 0.8997 | 814.5s\n",
    "-  epoch 02 | loss 0.0187 | dev_acc 0.8871 | dev_f1 0.8915 | 811.8s\n",
    "-  epoch 03 | loss 0.0070 | dev_acc 0.8931 | dev_f1 0.8936 | 812.9s\n",
    "-  early stop: no dev F1 improvement for 2 epoch(s).\n",
    "\n",
    "tokenizer_config.json: 100%\n",
    " 418/418 [00:00<00:00, 54.1kB/s]\n",
    "\n",
    "sentencepiece.bpe.model: 100%\n",
    " 5.07M/5.07M [00:01<00:00, 3.92MB/s]\n",
    "\n",
    "tokenizer.json: 100%\n",
    " 17.1M/17.1M [00:01<00:00, 14.1MB/s]\n",
    "\n",
    "special_tokens_map.json: 100%\n",
    " 280/280 [00:00<00:00, 36.4kB/s]\n",
    "\n",
    "config.json: 100%\n",
    " 694/694 [00:00<00:00, 88.7kB/s]\n",
    "\n",
    "model.safetensors: 100%\n",
    " 1.11G/1.11G [00:04<00:00, 441MB/s]\n",
    "\n",
    "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at intfloat/multilingual-e5-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
    "\n",
    "[run] e5 | frac=0.05 | train=5186 dev=4714 test=4270\n",
    "-  epoch 01 | loss 0.6743 | dev_acc 0.6655 | dev_f1 0.6992 | 50.3s\n",
    "-  epoch 02 | loss 0.4945 | dev_acc 0.7484 | dev_f1 0.7641 | 50.4s\n",
    "-  epoch 03 | loss 0.2965 | dev_acc 0.7679 | dev_f1 0.7799 | 50.1s\n",
    "-  epoch 04 | loss 0.1942 | dev_acc 0.7707 | dev_f1 0.7856 | 50.6s\n",
    "-  epoch 05 | loss 0.1405 | dev_acc 0.7811 | dev_f1 0.7851 | 50.2s\n",
    "\n",
    "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at intfloat/multilingual-e5-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
    "\n",
    "[run] e5 | frac=0.1 | train=10372 dev=4714 test=4270\n",
    "-  epoch 01 | loss 0.5921 | dev_acc 0.7650 | dev_f1 0.7465 | 89.2s\n",
    "-  epoch 02 | loss 0.3399 | dev_acc 0.8036 | dev_f1 0.8049 | 89.6s\n",
    "-  epoch 03 | loss 0.2139 | dev_acc 0.8065 | dev_f1 0.7966 | 89.7s\n",
    "-  epoch 04 | loss 0.1459 | dev_acc 0.8127 | dev_f1 0.8077 | 89.5s\n",
    "-  epoch 05 | loss 0.1049 | dev_acc 0.8127 | dev_f1 0.8083 | 90.0s\n",
    "\n",
    "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at intfloat/multilingual-e5-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
    "\n",
    "[run] e5 | frac=0.2 | train=20742 dev=4714 test=4270\n",
    "-  epoch 01 | loss 0.5105 | dev_acc 0.7966 | dev_f1 0.7822 | 169.1s\n",
    "-  epoch 02 | loss 0.2292 | dev_acc 0.8095 | dev_f1 0.8067 | 169.3s\n",
    "-  epoch 03 | loss 0.1411 | dev_acc 0.8303 | dev_f1 0.8285 | 169.5s\n",
    "-  epoch 04 | loss 0.0729 | dev_acc 0.8339 | dev_f1 0.8383 | 169.8s\n",
    "-  epoch 05 | loss 0.0404 | dev_acc 0.8360 | dev_f1 0.8387 | 169.3s\n",
    "\n",
    "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at intfloat/multilingual-e5-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
    "\n",
    "[run] e5 | frac=0.3 | train=31114 dev=4714 test=4270\n",
    "-  epoch 01 | loss 0.4667 | dev_acc 0.8154 | dev_f1 0.8099 | 248.1s\n",
    "-  epoch 02 | loss 0.1847 | dev_acc 0.8261 | dev_f1 0.8285 | 249.1s\n",
    "-  epoch 03 | loss 0.0857 | dev_acc 0.8375 | dev_f1 0.8443 | 249.6s\n",
    "-  epoch 04 | loss 0.0386 | dev_acc 0.8502 | dev_f1 0.8553 | 248.4s\n",
    "-  epoch 05 | loss 0.0133 | dev_acc 0.8464 | dev_f1 0.8499 | 248.3s\n",
    "\n",
    "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at intfloat/multilingual-e5-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
    "\n",
    "[run] e5 | frac=1.0 | train=103714 dev=4714 test=4270\n",
    "-  epoch 01 | loss 0.2843 | dev_acc 0.8541 | dev_f1 0.8583 | 801.1s\n",
    "-  epoch 02 | loss 0.0379 | dev_acc 0.8513 | dev_f1 0.8512 | 803.1s\n",
    "-  epoch 03 | loss 0.0118 | dev_acc 0.8430 | dev_f1 0.8521 | 802.4s\n",
    "-  early stop: no dev F1 improvement for 2 epoch(s).\n",
    "\n",
    "config.json: 100%\n",
    " 491/491 [00:00<00:00, 63.9kB/s]\n",
    "\n",
    "vocab.txt: \n",
    " 2.24M/? [00:00<00:00, 58.3MB/s]\n",
    "\n",
    "model.safetensors: 100%\n",
    " 660M/660M [00:01<00:00, 433MB/s]\n",
    "\n",
    "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sagorsarker/bangla-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
    "\n",
    "[run] banglabert | frac=0.05 | train=5186 dev=4714 test=4270\n",
    "-  epoch 01 | loss 0.6050 | dev_acc 0.7367 | dev_f1 0.7250 | 43.9s\n",
    "-  epoch 02 | loss 0.3134 | dev_acc 0.7552 | dev_f1 0.7602 | 43.9s\n",
    "-  epoch 03 | loss 0.1632 | dev_acc 0.7563 | dev_f1 0.7608 | 43.9s\n",
    "-  epoch 04 | loss 0.0815 | dev_acc 0.7592 | dev_f1 0.7631 | 43.7s\n",
    "-  epoch 05 | loss 0.0368 | dev_acc 0.7650 | dev_f1 0.7610 | 43.8s\n",
    "\n",
    "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sagorsarker/bangla-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
    "\n",
    "[run] banglabert | frac=0.1 | train=10372 dev=4714 test=4270\n",
    "-  epoch 01 | loss 0.5391 | dev_acc 0.7603 | dev_f1 0.7354 | 77.7s\n",
    "-  epoch 02 | loss 0.2399 | dev_acc 0.7764 | dev_f1 0.7846 | 77.4s\n",
    "-  epoch 03 | loss 0.1205 | dev_acc 0.7720 | dev_f1 0.7623 | 77.4s\n",
    "-  epoch 04 | loss 0.0671 | dev_acc 0.7813 | dev_f1 0.7676 | 77.5s\n",
    "-  early stop: no dev F1 improvement for 2 epoch(s).\n",
    "\n",
    "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sagorsarker/bangla-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
    "\n",
    "[run] banglabert | frac=0.2 | train=20742 dev=4714 test=4270\n",
    "-  epoch 01 | loss 0.4503 | dev_acc 0.7864 | dev_f1 0.7847 | 145.5s\n",
    "-  epoch 02 | loss 0.1583 | dev_acc 0.7951 | dev_f1 0.7934 | 145.8s\n",
    "-  epoch 03 | loss 0.0587 | dev_acc 0.8055 | dev_f1 0.8031 | 146.0s\n",
    "-  epoch 04 | loss 0.0210 | dev_acc 0.7995 | dev_f1 0.8047 | 145.6s\n",
    "-  epoch 05 | loss 0.0073 | dev_acc 0.8036 | dev_f1 0.8088 | 145.1s\n",
    "\n",
    "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sagorsarker/bangla-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
    "\n",
    "[run] banglabert | frac=0.3 | train=31114 dev=4714 test=4270\n",
    "-  epoch 01 | loss 0.3921 | dev_acc 0.7864 | dev_f1 0.7712 | 214.2s\n",
    "-  epoch 02 | loss 0.1022 | dev_acc 0.8038 | dev_f1 0.8103 | 213.1s\n",
    "-  epoch 03 | loss 0.0309 | dev_acc 0.8108 | dev_f1 0.8151 | 213.5s\n",
    "-  epoch 04 | loss 0.0150 | dev_acc 0.8127 | dev_f1 0.8156 | 213.7s\n",
    "-  epoch 05 | loss 0.0037 | dev_acc 0.8159 | dev_f1 0.8195 | 213.4s\n",
    "\n",
    "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sagorsarker/bangla-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
    "\n",
    "[run] banglabert | frac=1.0 | train=103714 dev=4714 test=4270\n",
    "-  epoch 01 | loss 0.2225 | dev_acc 0.8023 | dev_f1 0.8169 | 686.4s\n",
    "-  epoch 02 | loss 0.0230 | dev_acc 0.8063 | dev_f1 0.8066 | 687.4s\n",
    "-  epoch 03 | loss 0.0071 | dev_acc 0.8129 | dev_f1 0.8201 | 686.8s\n",
    "-  epoch 04 | loss 0.0025 | dev_acc 0.8017 | dev_f1 0.8176 | 688.1s\n",
    "-  epoch 05 | loss 0.0009 | dev_acc 0.8106 | dev_f1 0.8218 | 686.3s\n",
    "\n",
    "Final (test) results\n",
    "\n",
    "          model..regime_frac..test_acc..test_f1\n",
    "0)   banglabert_________0.05__0.740281__0.745584\n",
    "1)   banglabert_________0.10__0.763700__0.779550\n",
    "2)   banglabert_________0.20__0.798829__0.805788\n",
    "3)   banglabert_________0.30__0.799063__0.804467\n",
    "4)   banglabert_________1.00__0.791335__0.807684\n",
    "5)   e5__________________0.05__0.775176__0.790026\n",
    "6)   e5__________________0.10__0.804450__0.803391\n",
    "7)   e5__________________0.20__0.832084__0.837966\n",
    "8)   e5__________________0.30__0.842623__0.849462\n",
    "9)   e5__________________1.00__0.846136__0.852658\n",
    "10)  labse_______________0.05__0.797892__0.798506\n",
    "11)  labse_______________0.10__0.836534__0.843146\n",
    "12)  labse_______________0.20__0.867681__0.870204\n",
    "13)  labse_______________0.30__0.888993__0.891085\n",
    "14)  labse_______________1.00__0.884543__0.885800\n",
    "15)  muril_______________0.05__0.797892__0.796510\n",
    "16)  muril_______________0.10__0.826932__0.828658\n",
    "17)  muril_______________0.20__0.869087__0.870452\n",
    "18)  muril_______________0.30__0.876347__0.875413\n",
    "19)  muril_______________1.00__0.881499__0.883141\n",
    "20)  sahajbert___________0.05__0.784543__0.783936\n",
    "21)  sahajbert___________0.10__0.830679__0.838436\n",
    "22)  sahajbert___________0.20__0.854333__0.861470\n",
    "23)  sahajbert___________0.30__0.863700__0.872424\n",
    "24)  sahajbert___________1.00__0.885714__0.891169"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff0dc0f",
   "metadata": {},
   "source": [
    "**Few-/Full-Shot Fine-Tuning for Bangla WiC with a Trainer-Free PyTorch Loop (on Capped WIC dataset)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604c4ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] transformers version: 4.55.0\n",
      "[skip] already finished: sahajbert_frac1 (test_f1=0.4329896907216495)\n",
      "[skip] already finished: sahajbert_frac5 (test_f1=0.6220095693779905)\n",
      "[skip] already finished: sahajbert_frac10 (test_f1=0.5806451612903226)\n",
      "[skip] already finished: sahajbert_frac20 (test_f1=0.7302904564315352)\n",
      "[skip] already finished: sahajbert_frac30 (test_f1=0.6857142857142857)\n",
      "[skip] already finished: sahajbert_frac100 (test_f1=0.734375)\n",
      "[skip] already finished: muril_frac1 (test_f1=0.6666666666666666)\n",
      "[skip] already finished: muril_frac5 (test_f1=0.662020905923345)\n",
      "[skip] already finished: muril_frac10 (test_f1=0.6666666666666666)\n",
      "[skip] already finished: muril_frac20 (test_f1=0.6890756302521008)\n",
      "[skip] already finished: muril_frac30 (test_f1=0.7083333333333334)\n",
      "[skip] already finished: muril_frac100 (test_f1=0.7175572519083969)\n",
      "[skip] already finished: labse_frac1 (test_f1=0.6167400881057269)\n",
      "[skip] already finished: labse_frac5 (test_f1=0.5520361990950227)\n",
      "[skip] already finished: labse_frac10 (test_f1=0.6788990825688074)\n",
      "[skip] already finished: labse_frac20 (test_f1=0.6981132075471698)\n",
      "[resume:finalize] found best for labse_frac30; running test only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/LaBSE and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[run] labse | frac=1.0 | train=5106 dev=218 test=194 (start_epoch=1)\n",
      "  epoch 01 | loss 0.5212 | dev_acc 0.5321 | dev_f1 0.6792 | 2268.9s\n",
      "  epoch 02 | loss 0.1671 | dev_acc 0.6697 | dev_f1 0.7097 | 2576.0s\n",
      "  epoch 03 | loss 0.0549 | dev_acc 0.6789 | dev_f1 0.7222 | 2340.7s\n",
      "  epoch 04 | loss 0.0221 | dev_acc 0.6468 | dev_f1 0.7200 | 2497.6s\n",
      "  epoch 05 | loss 0.0103 | dev_acc 0.6468 | dev_f1 0.7200 | 2516.1s\n",
      "  early stop: no dev F1 improvement for 2 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at intfloat/multilingual-e5-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[run] e5 | frac=0.01 | train=52 dev=218 test=194 (start_epoch=1)\n",
      "  epoch 01 | loss 0.7019 | dev_acc 0.5000 | dev_f1 0.6175 | 45.9s\n",
      "  epoch 02 | loss 0.6851 | dev_acc 0.5367 | dev_f1 0.5121 | 44.9s\n",
      "  epoch 03 | loss 0.6500 | dev_acc 0.4771 | dev_f1 0.1618 | 45.6s\n",
      "  early stop: no dev F1 improvement for 2 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at intfloat/multilingual-e5-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[run] e5 | frac=0.05 | train=256 dev=218 test=194 (start_epoch=1)\n",
      "  epoch 01 | loss 0.7178 | dev_acc 0.4817 | dev_f1 0.2098 | 143.2s\n",
      "  epoch 02 | loss 0.6744 | dev_acc 0.5138 | dev_f1 0.6558 | 130.8s\n",
      "  epoch 03 | loss 0.6332 | dev_acc 0.5138 | dev_f1 0.4804 | 140.4s\n",
      "  epoch 04 | loss 0.5858 | dev_acc 0.5092 | dev_f1 0.5023 | 145.7s\n",
      "  early stop: no dev F1 improvement for 2 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at intfloat/multilingual-e5-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[run] e5 | frac=0.1 | train=510 dev=218 test=194 (start_epoch=1)\n",
      "  epoch 01 | loss 0.7058 | dev_acc 0.4587 | dev_f1 0.3295 | 250.8s\n",
      "  epoch 02 | loss 0.6717 | dev_acc 0.5046 | dev_f1 0.5424 | 241.0s\n",
      "  epoch 03 | loss 0.6184 | dev_acc 0.4817 | dev_f1 0.5498 | 241.7s\n",
      "  epoch 04 | loss 0.5576 | dev_acc 0.5183 | dev_f1 0.5783 | 242.8s\n",
      "  epoch 05 | loss 0.4955 | dev_acc 0.5138 | dev_f1 0.6015 | 239.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at intfloat/multilingual-e5-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[run] e5 | frac=0.2 | train=1022 dev=218 test=194 (start_epoch=1)\n",
      "  epoch 01 | loss 0.6873 | dev_acc 0.5046 | dev_f1 0.6687 | 456.2s\n",
      "  epoch 02 | loss 0.6039 | dev_acc 0.5688 | dev_f1 0.6643 | 444.5s\n",
      "  epoch 03 | loss 0.4523 | dev_acc 0.6193 | dev_f1 0.6029 | 442.7s\n",
      "  early stop: no dev F1 improvement for 2 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at intfloat/multilingual-e5-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[run] e5 | frac=0.3 | train=1532 dev=218 test=194 (start_epoch=1)\n",
      "  epoch 01 | loss 0.6889 | dev_acc 0.5275 | dev_f1 0.5422 | 645.3s\n",
      "  epoch 02 | loss 0.5338 | dev_acc 0.5642 | dev_f1 0.6468 | 625.0s\n",
      "  epoch 03 | loss 0.3177 | dev_acc 0.6055 | dev_f1 0.6884 | 636.9s\n",
      "  epoch 04 | loss 0.1915 | dev_acc 0.6606 | dev_f1 0.7218 | 621.5s\n",
      "  epoch 05 | loss 0.1052 | dev_acc 0.6606 | dev_f1 0.7109 | 618.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at intfloat/multilingual-e5-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[run] e5 | frac=1.0 | train=5106 dev=218 test=194 (start_epoch=1)\n",
      "  epoch 01 | loss 0.5925 | dev_acc 0.6101 | dev_f1 0.6352 | 2102.8s\n",
      "  epoch 02 | loss 0.2400 | dev_acc 0.6101 | dev_f1 0.6863 | 2136.0s\n",
      "  epoch 03 | loss 0.1160 | dev_acc 0.6147 | dev_f1 0.7000 | 2108.6s\n",
      "  epoch 04 | loss 0.0691 | dev_acc 0.6284 | dev_f1 0.7216 | 2146.3s\n",
      "  epoch 05 | loss 0.0293 | dev_acc 0.6376 | dev_f1 0.7228 | 2127.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sagorsarker/bangla-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[run] banglabert | frac=0.01 | train=52 dev=218 test=194 (start_epoch=1)\n",
      "  epoch 01 | loss 0.6991 | dev_acc 0.4908 | dev_f1 0.5277 | 39.6s\n",
      "  epoch 02 | loss 0.5935 | dev_acc 0.4908 | dev_f1 0.5394 | 37.0s\n",
      "  epoch 03 | loss 0.5742 | dev_acc 0.5092 | dev_f1 0.5244 | 37.2s\n",
      "  epoch 04 | loss 0.4696 | dev_acc 0.5183 | dev_f1 0.4670 | 37.7s\n",
      "  early stop: no dev F1 improvement for 2 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sagorsarker/bangla-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[run] banglabert | frac=0.05 | train=256 dev=218 test=194 (start_epoch=1)\n",
      "  epoch 01 | loss 0.6948 | dev_acc 0.5046 | dev_f1 0.6687 | 107.8s\n",
      "  epoch 02 | loss 0.6173 | dev_acc 0.6193 | dev_f1 0.6103 | 92.8s\n",
      "  epoch 03 | loss 0.4496 | dev_acc 0.6239 | dev_f1 0.6940 | 107.9s\n",
      "  epoch 04 | loss 0.3373 | dev_acc 0.6789 | dev_f1 0.7083 | 125.8s\n",
      "  epoch 05 | loss 0.2548 | dev_acc 0.6606 | dev_f1 0.7016 | 122.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sagorsarker/bangla-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[run] banglabert | frac=0.1 | train=510 dev=218 test=194 (start_epoch=1)\n",
      "  epoch 01 | loss 0.7182 | dev_acc 0.5596 | dev_f1 0.6757 | 202.0s\n",
      "  epoch 02 | loss 0.5454 | dev_acc 0.6881 | dev_f1 0.7444 | 208.4s\n",
      "  epoch 03 | loss 0.3634 | dev_acc 0.6743 | dev_f1 0.7300 | 179.2s\n",
      "  epoch 04 | loss 0.2017 | dev_acc 0.6927 | dev_f1 0.7173 | 159.9s\n",
      "  early stop: no dev F1 improvement for 2 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sagorsarker/bangla-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[run] banglabert | frac=0.2 | train=1022 dev=218 test=194 (start_epoch=1)\n",
      "  epoch 01 | loss 0.6670 | dev_acc 0.6422 | dev_f1 0.6100 | 333.4s\n",
      "  epoch 02 | loss 0.3805 | dev_acc 0.6743 | dev_f1 0.7149 | 330.1s\n",
      "  epoch 03 | loss 0.1623 | dev_acc 0.6514 | dev_f1 0.6960 | 373.5s\n",
      "  epoch 04 | loss 0.0702 | dev_acc 0.6606 | dev_f1 0.7063 | 379.8s\n",
      "  early stop: no dev F1 improvement for 2 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sagorsarker/bangla-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[run] banglabert | frac=0.3 | train=1532 dev=218 test=194 (start_epoch=1)\n",
      "  epoch 01 | loss 0.6394 | dev_acc 0.6422 | dev_f1 0.6977 | 514.5s\n",
      "  epoch 02 | loss 0.3249 | dev_acc 0.6560 | dev_f1 0.7148 | 512.0s\n",
      "  epoch 03 | loss 0.1454 | dev_acc 0.6606 | dev_f1 0.6992 | 522.5s\n",
      "  epoch 04 | loss 0.0549 | dev_acc 0.6835 | dev_f1 0.7206 | 504.5s\n",
      "  epoch 05 | loss 0.0183 | dev_acc 0.6697 | dev_f1 0.7000 | 516.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sagorsarker/bangla-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[run] banglabert | frac=1.0 | train=5106 dev=218 test=194 (start_epoch=1)\n",
      "  epoch 01 | loss 0.4836 | dev_acc 0.6560 | dev_f1 0.7331 | 1779.0s\n",
      "  epoch 02 | loss 0.1595 | dev_acc 0.6743 | dev_f1 0.7171 | 1776.1s\n",
      "  epoch 03 | loss 0.0585 | dev_acc 0.6927 | dev_f1 0.7491 | 1766.5s\n",
      "  epoch 04 | loss 0.0198 | dev_acc 0.6927 | dev_f1 0.7452 | 1778.6s\n",
      "  epoch 05 | loss 0.0088 | dev_acc 0.6881 | dev_f1 0.7500 | 1779.6s\n",
      "\n",
      "== Final (test) results ==\n",
      "          model  regime_frac  test_acc   test_f1\n",
      "0   banglabert         0.01  0.505155  0.596639\n",
      "1   banglabert         0.05  0.623711  0.675556\n",
      "2   banglabert         0.10  0.613402  0.683544\n",
      "3   banglabert         0.20  0.664948  0.716157\n",
      "4   banglabert         0.30  0.685567  0.726457\n",
      "5   banglabert         1.00  0.634021  0.719368\n",
      "6           e5         0.01  0.469072  0.579592\n",
      "7           e5         0.05  0.510309  0.649446\n",
      "8           e5         0.10  0.577320  0.637168\n",
      "9           e5         0.20  0.494845  0.662069\n",
      "10          e5         0.30  0.680412  0.728070\n",
      "11          e5         1.00  0.680412  0.745902\n",
      "12       labse         0.01  0.551546  0.616740\n",
      "13       labse         0.05  0.489691  0.552036\n",
      "14       labse         0.10  0.639175  0.678899\n",
      "15       labse         0.20  0.670103  0.698113\n",
      "16       labse         0.30  0.644330  0.708861\n",
      "17       labse         1.00  0.701031  0.728972\n",
      "18       muril         0.01  0.500000  0.666667\n",
      "19       muril         0.05  0.500000  0.662021\n",
      "20       muril         0.10  0.500000  0.666667\n",
      "21       muril         0.20  0.618557  0.689076\n",
      "22       muril         0.30  0.639175  0.708333\n",
      "23       muril         1.00  0.618557  0.717557\n",
      "24   sahajbert         0.01  0.432990  0.432990\n",
      "25   sahajbert         0.05  0.592784  0.622010\n",
      "26   sahajbert         0.10  0.530928  0.580645\n",
      "27   sahajbert         0.20  0.664948  0.730290\n",
      "28   sahajbert         0.30  0.659794  0.685714\n",
      "29   sahajbert         1.00  0.649485  0.734375\n"
     ]
    }
   ],
   "source": [
    "import os, json, random, time                               # Std: filesystem, JSON I/O, RNG seeding, timing\n",
    "from dataclasses import dataclass                           # For lightweight dataset container\n",
    "from typing import Dict, List                               # Type hints for clarity\n",
    "import numpy as np                                          # Numerical ops for logits → metrics\n",
    "import pandas as pd                                         # Tabular wrangling of WiC lists\n",
    "import torch                                                # Core PyTorch\n",
    "from torch.utils.data import DataLoader                     # Mini-batching utilities\n",
    "from sklearn.metrics import accuracy_score, f1_score        # Standard classification metrics\n",
    "\n",
    "import transformers                                         # HF Transformers library (version printed below)\n",
    "from transformers import (                                  # Selected utilities/classes from Transformers\n",
    "    AutoTokenizer, AutoConfig, AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding, get_linear_schedule_with_warmup, set_seed\n",
    ")\n",
    "\n",
    "print(\"[info] transformers version:\", transformers.__version__)  # Log Transformers version for reproducibility\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "DATA_DIR = r\"C:\\Users\\Student\\Downloads\\projecting_sentences-main\\projecting_sentences-main\\Dataset\\bangla_wic_dataset_capped\"  # Root folder containing wic_train/dev/test.json\n",
    "OUT_DIR  = r\"C:\\Users\\Student\\Downloads\\projecting_sentences-main\\projecting_sentences-main\\Result\\WIC_finetuned_result_capped\"   # Output root for checkpoints & CSVs\n",
    "\n",
    "MODELS = {     # HF model id per short name to iterate over\n",
    "    \"sahajbert\": \"neuropark/sahajBERT\",\n",
    "    \"muril\"    : \"google/muril-base-cased\",\n",
    "    \"labse\"    : \"sentence-transformers/LaBSE\",\n",
    "    \"e5\"       : \"intfloat/multilingual-e5-base\",\n",
    "    \"banglabert\":\"sagorsarker/bangla-bert-base\",\n",
    "    # Note: all are fine-tuned as sequence classifiers\n",
    "}\n",
    "\n",
    "REGIMES = [0.05, 0.10, 0.20, 0.30, 1.00]                     # Few-shot fractions and full-shot (100%)\n",
    "\n",
    "SEED = 42                                                    # Global reproducibility seed\n",
    "BATCH_SIZE = 16                                              # Per-step batch size\n",
    "MAX_EPOCHS = 5                                               # Maximum epochs before stopping\n",
    "LR = 2e-5                                                    # AdamW learning rate\n",
    "WARMUP_RATIO = 0.06                                          # Linear warmup proportion of total steps\n",
    "PATIENCE = 2                                                 # Early-stopping: epochs without dev-F1 improvement\n",
    "MAX_LEN = 256                                                # Max sequence length for tokenization\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Prefer GPU if available\n",
    "\n",
    "\n",
    "# IO & UTILS\n",
    "\n",
    "# Function: load_json_array\n",
    "# Purpose: Read a WiC split from disk (JSON list of dicts) and return it as a Python list.\n",
    "# Inputs:\n",
    "#   path (str): filesystem path to a JSON file containing the WiC examples.\n",
    "# Outputs:\n",
    "#   List[Dict]: a Python list where each element is a dictionary for one WiC example.\n",
    "def load_json_array(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:            # Open the JSON file in UTF-8 mode\n",
    "        return json.load(f)                                  # Parse and return as Python list\n",
    "\n",
    "\n",
    "# Function: with_markers\n",
    "# Purpose: Insert explicit [TGT] and [/TGT] markers into a sentence around a target span.\n",
    "# Inputs:\n",
    "#   sentence (str): the full sentence text.\n",
    "#   start (int): character start index (inclusive) of the target span.\n",
    "#   end (int): character end index (exclusive) of the target span.\n",
    "#   open_tok (str): left marker token (default \"[TGT]\").\n",
    "#   close_tok (str): right marker token (default \"[/TGT]\").\n",
    "# Outputs:\n",
    "#   str: the sentence with markers injected around the specified span (clamped to safe range).\n",
    "def with_markers(sentence: str, start: int, end: int, open_tok=\"[TGT]\", close_tok=\"[/TGT]\"):\n",
    "    start = max(0, int(start)); end = max(start, int(end))   # Clamp indices and enforce start ≤ end\n",
    "    return sentence[:start] + open_tok + sentence[start:end] + close_tok + sentence[end:]  # Return marked sentence\n",
    "\n",
    "\n",
    "# Function: to_dataframe\n",
    "# Purpose: Convert a raw WiC JSON list into a tidy DataFrame with standardized columns for training.\n",
    "# Inputs:\n",
    "#   wic_list (List[Dict]): list of WiC examples read from JSON.\n",
    "# Outputs:\n",
    "#   pd.DataFrame: columns = lemma, s1, s2, sid1, sid2, start1, end1, start2, end2, label (int).\n",
    "def to_dataframe(wic_list: List[Dict]):\n",
    "    rows = []                                                # Accumulator for row dicts\n",
    "    for x in wic_list:                                       # Iterate over raw examples\n",
    "        rows.append({\n",
    "            \"lemma\": x[\"lemma\"],                             # Lemma string\n",
    "            \"s1\": x[\"sentence1\"],                            # Left sentence\n",
    "            \"s2\": x[\"sentence2\"],                            # Right sentence\n",
    "            \"sid1\": x[\"sent_id1\"],                           # Left sentence id\n",
    "            \"sid2\": x[\"sent_id2\"],                           # Right sentence id\n",
    "            \"start1\": int(x[\"start1\"]), \"end1\": int(x[\"end1\"]),  # Left target span (char offsets)\n",
    "            \"start2\": int(x[\"start2\"]), \"end2\": int(x[\"end2\"]),  # Right target span (char offsets)\n",
    "            \"label\": int(x[\"label\"]),                        # Gold label (1 = same sense, 0 = different)\n",
    "        })\n",
    "    return pd.DataFrame(rows)                                # Build DataFrame from rows\n",
    "\n",
    "\n",
    "# Function: stratified_fraction\n",
    "# Purpose: Take a stratified sample of the DataFrame by label to achieve a given fraction for few-shot regimes.\n",
    "# Inputs:\n",
    "#   df (pd.DataFrame): full training DataFrame with a 'label' column.\n",
    "#   frac (float): fraction to sample; if ≥ 1.0, return a full shuffle.\n",
    "#   seed (int): RNG seed for reproducibility.\n",
    "# Outputs:\n",
    "#   pd.DataFrame: sampled DataFrame with near-constant label proportions.\n",
    "def stratified_fraction(df: pd.DataFrame, frac: float, seed: int):\n",
    "    if frac >= 1.0:                                          # Full-shot case\n",
    "        return df.sample(frac=1.0, random_state=seed).reset_index(drop=True)  # Return shuffled copy\n",
    "    parts = []                                               # Accumulate per-class samples\n",
    "    for y, sub in df.groupby(\"label\"):                       # Split by class label\n",
    "        k = max(1, int(round(frac * len(sub))))              # Class-size specific sample count\n",
    "        parts.append(sub.sample(n=k, random_state=seed))     # Sample that many rows from this class\n",
    "    return pd.concat(parts, axis=0).sample(frac=1.0, random_state=seed).reset_index(drop=True)  # Recombine & shuffle\n",
    "\n",
    "\n",
    "# Dataclass: WiCDataset\n",
    "# Purpose: Minimal Dataset wrapper holding token encodings and labels for the Trainer-free loop.\n",
    "# Fields:\n",
    "#   encodings (Dict[str, List[List[int]]]): tokenized features (input_ids, attention_mask, etc.) as lists.\n",
    "#   labels (List[int]): gold labels aligned to encodings.\n",
    "@dataclass\n",
    "class WiCDataset(torch.utils.data.Dataset):\n",
    "    encodings: Dict[str, List[List[int]]]                    # Token features dictionary\n",
    "    labels: List[int]                                        # Gold labels per example\n",
    "    def __len__(self): return len(self.labels)               # Return dataset size\n",
    "    def __getitem__(self, idx):                              # Retrieve one item by index\n",
    "        # return python lists; collator will pad & convert to tensors\n",
    "        item = {k: v[idx] for k, v in self.encodings.items()}  # Slice each feature list\n",
    "        item[\"labels\"] = self.labels[idx]                    # Attach the corresponding label\n",
    "        return item                                          # Return a dict expected by the model\n",
    "\n",
    "\n",
    "# Function: tokenize_pairs\n",
    "# Purpose: Insert markers into sentence pairs and tokenize them with a given tokenizer.\n",
    "# Inputs:\n",
    "#   df (pd.DataFrame): must include s1, s2, start1, end1, start2, end2, and label.\n",
    "#   tokenizer: a Hugging Face tokenizer instance.\n",
    "# Outputs:\n",
    "#   (encodings, labels): encodings is a dict of token lists; labels is a list[int].\n",
    "def tokenize_pairs(df: pd.DataFrame, tokenizer):\n",
    "    OPEN, CLOSE = \"[TGT]\", \"[/TGT]\"                          # Consistent marker tokens\n",
    "    s1 = [with_markers(a, s, e, OPEN, CLOSE) for a, s, e in zip(df[\"s1\"], df[\"start1\"], df[\"end1\"])]  # Left marked\n",
    "    s2 = [with_markers(b, s, e, OPEN, CLOSE) for b, s, e in zip(df[\"s2\"], df[\"start2\"], df[\"end2\"])]  # Right marked\n",
    "    enc = tokenizer(                                         # Tokenize sentence pairs\n",
    "        s1, s2,\n",
    "        truncation=True,                                     # Truncate to MAX_LEN\n",
    "        max_length=MAX_LEN,\n",
    "        padding=False                                        # Dynamic padding handled by collator\n",
    "    )\n",
    "    labels = df[\"label\"].astype(int).tolist()                # Extract labels as ints\n",
    "    return enc, labels                                       # Return features and labels\n",
    "\n",
    "\n",
    "# Function: compute_metrics_from_logits\n",
    "# Purpose: Convert raw model logits to predictions and compute accuracy and F1.\n",
    "# Inputs:\n",
    "#   logits (np.ndarray): shape [N, 2] for binary classification.\n",
    "#   labels (List[int]): gold labels (0/1) of length N.\n",
    "# Outputs:\n",
    "#   (acc, f1): tuple of floats with accuracy and F1 score.\n",
    "def compute_metrics_from_logits(logits: np.ndarray, labels: List[int]):\n",
    "    preds = logits.argmax(axis=-1)                           # Predicted class = argmax over logits\n",
    "    acc = accuracy_score(labels, preds)                      # Compute accuracy\n",
    "    f1  = f1_score(labels, preds)                            # Compute F1 (binary average='binary')\n",
    "    return acc, f1                                           # Return both metrics\n",
    "\n",
    "\n",
    "# TRAIN / EVAL\n",
    "\n",
    "# Function: epoch_eval\n",
    "# Purpose: Evaluate a model over a dataloader and compute accuracy and F1 from accumulated logits.\n",
    "# Inputs:\n",
    "#   model (nn.Module): sequence classification model.\n",
    "#   dataloader (DataLoader): batched dataset iterator.\n",
    "#   device (torch.device): 'cuda' or 'cpu' to run inference on.\n",
    "# Outputs:\n",
    "#   (acc, f1): floats with accuracy and F1 on the provided dataloader.\n",
    "def epoch_eval(model, dataloader, device):\n",
    "    model.eval()                                             # Switch to eval mode (no dropout, etc.)\n",
    "    all_logits = []                                          # Accumulator for logits\n",
    "    all_labels = []                                          # Accumulator for gold labels\n",
    "    with torch.no_grad():                                    # Disable gradient tracking\n",
    "        for batch in dataloader:                             # Iterate over batches\n",
    "            labels = batch.pop(\"labels\").to(device)          # Move labels to device and remove from features\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}  # Move all features to device\n",
    "            outputs = model(**batch)                         # Forward pass\n",
    "            logits = outputs.logits.detach().cpu().numpy()   # Collect logits on CPU as NumPy\n",
    "            all_logits.append(logits)                        # Append batch logits\n",
    "            all_labels.append(labels.cpu().numpy())          # Append batch labels\n",
    "    all_logits = np.concatenate(all_logits, axis=0) if all_logits else np.zeros((0,2))  # Stack logits\n",
    "    all_labels = np.concatenate(all_labels, axis=0).astype(int) if all_labels else np.zeros((0,), dtype=int)  # Stack labels\n",
    "    return compute_metrics_from_logits(all_logits, all_labels)  # Compute and return metrics\n",
    "\n",
    "\n",
    "# Function: train_eval_one\n",
    "# Purpose: Fine-tune one backbone at a given regime, with early stopping on dev F1; save best; evaluate on test.\n",
    "# Inputs:\n",
    "#   model_id (str): Hugging Face model identifier.\n",
    "#   model_name (str): short alias for naming outputs.\n",
    "#   df_train, df_dev, df_test (pd.DataFrame): split dataframes from WiC JSONs.\n",
    "#   regime_frac (float): fraction of the training set to sample (few-/full-shot).\n",
    "#   out_root (str): output directory root for this model/regime run.\n",
    "# Outputs:\n",
    "#   Dict: a single summary row containing sizes, best dev F1, and test metrics.\n",
    "def train_eval_one(model_id, model_name, df_train, df_dev, df_test, regime_frac, out_root):\n",
    "    set_seed(SEED)                                           # Set HF/torch/random seeds\n",
    "\n",
    "    # Tokenizer & model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)  # Load tokenizer\n",
    "    tokenizer.add_special_tokens({\"additional_special_tokens\": [\"[TGT]\", \"[/TGT]\"]})  # Add markers to vocab\n",
    "\n",
    "    config = AutoConfig.from_pretrained(model_id, num_labels=2)  # Binary classification head\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_id, config=config)  # Load base weights\n",
    "    model.resize_token_embeddings(len(tokenizer))            # Resize embeddings due to added tokens\n",
    "    model.to(DEVICE)                                        # Move model to GPU/CPU\n",
    "\n",
    "    # Few-shot sample\n",
    "    df_subtrain = stratified_fraction(df_train, regime_frac, seed=SEED)  # Stratified downsample of train\n",
    "\n",
    "    # Tokenize\n",
    "    enc_tr, y_tr = tokenize_pairs(df_subtrain, tokenizer)    # Tokenize sampled train\n",
    "    enc_dv, y_dv = tokenize_pairs(df_dev, tokenizer)         # Tokenize dev\n",
    "    enc_ts, y_ts = tokenize_pairs(df_test, tokenizer)        # Tokenize test\n",
    "\n",
    "    # Datasets / Loaders\n",
    "    collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8 if torch.cuda.is_available() else None)  # Dynamic pad\n",
    "    ds_tr = WiCDataset(enc_tr, y_tr)                         # Train dataset\n",
    "    ds_dv = WiCDataset(enc_dv, y_dv)                         # Dev dataset\n",
    "    ds_ts = WiCDataset(enc_ts, y_ts)                         # Test dataset\n",
    "\n",
    "    train_loader = DataLoader(ds_tr, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collator, num_workers=0)   # Train loader\n",
    "    dev_loader   = DataLoader(ds_dv, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collator, num_workers=0)  # Dev loader\n",
    "    test_loader  = DataLoader(ds_ts, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collator, num_workers=0)  # Test loader\n",
    "\n",
    "    # Optim / Scheduler\n",
    "    total_steps = max(1, len(train_loader) * MAX_EPOCHS)     # Total train steps for scheduler\n",
    "    warmup_steps = int(WARMUP_RATIO * total_steps)           # Num warmup steps\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR) # AdamW optimizer\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)  # Linear warmup/decay\n",
    "\n",
    "    # Output dir\n",
    "    tag = f\"{model_name}_frac{int(regime_frac*100)}\"         # Tag includes model alias and regime\n",
    "    out_dir = os.path.join(out_root, tag); os.makedirs(out_dir, exist_ok=True)  # Make output folder\n",
    "\n",
    "    best_dev_f1 = -1.0                                       # Track best dev F1 for early stopping\n",
    "    no_improve = 0                                           # Counter for patience\n",
    "    history = []                                             # Per-epoch logs\n",
    "\n",
    "    print(f\"\\n[run] {model_name} | frac={regime_frac} | train={len(ds_tr)} dev={len(ds_dv)} test={len(ds_ts)}\")  # Run header\n",
    "\n",
    "    for epoch in range(1, MAX_EPOCHS+1):                     # Epoch loop\n",
    "        model.train()                                        # Train mode\n",
    "        t0 = time.time()                                     # Epoch timer\n",
    "        total_loss = 0.0                                     # Reset loss accumulator\n",
    "        for batch in train_loader:                           # Iterate training batches\n",
    "            labels = batch.pop(\"labels\").to(DEVICE)          # Extract and move labels\n",
    "            batch  = {k: v.to(DEVICE) for k, v in batch.items()}  # Move features to device\n",
    "\n",
    "            outputs = model(**batch, labels=labels)          # Forward pass with labels\n",
    "            loss = outputs.loss                              # Cross-entropy loss\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)            # Clear grads\n",
    "            loss.backward()                                  # Backprop\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping for stability\n",
    "            optimizer.step()                                 # Optimizer step\n",
    "            scheduler.step()                                 # Scheduler step\n",
    "\n",
    "            total_loss += loss.item()                        # Accumulate batch loss\n",
    "\n",
    "        # Eval on dev each epoch\n",
    "        dev_acc, dev_f1 = epoch_eval(model, dev_loader, DEVICE)  # Validate on dev\n",
    "        avg_loss = total_loss / max(1, len(train_loader))    # Compute mean train loss for epoch\n",
    "        history.append({\"epoch\": epoch, \"train_loss\": avg_loss, \"dev_acc\": dev_acc, \"dev_f1\": dev_f1})  # Log row\n",
    "        print(f\"  epoch {epoch:02d} | loss {avg_loss:.4f} | dev_acc {dev_acc:.4f} | dev_f1 {dev_f1:.4f} | {time.time()-t0:.1f}s\")  # Epoch summary\n",
    "\n",
    "        # Early stopping on dev F1\n",
    "        if dev_f1 > best_dev_f1:                             # If improved dev F1\n",
    "            best_dev_f1 = dev_f1                             # Update best\n",
    "            no_improve = 0                                   # Reset patience counter\n",
    "            # Save best\n",
    "            model.save_pretrained(out_dir)                   # Persist model weights/config\n",
    "            tokenizer.save_pretrained(out_dir)               # Persist tokenizer artifacts\n",
    "            with open(os.path.join(out_dir, \"dev_best.json\"), \"w\", encoding=\"utf-8\") as f:  # Save best dev metrics\n",
    "                json.dump({\"epoch\": epoch, \"dev_acc\": dev_acc, \"dev_f1\": dev_f1}, f, ensure_ascii=False, indent=2)\n",
    "        else:\n",
    "            no_improve += 1                                  # No improvement this epoch\n",
    "            if no_improve >= PATIENCE:                       # Hit patience threshold\n",
    "                print(f\"  early stop: no dev F1 improvement for {PATIENCE} epoch(s).\")  # Log early stop\n",
    "                break                                        # Exit training loop\n",
    "\n",
    "    # Load best (already saved) and evaluate on test\n",
    "    # (Re-use in-memory 'model' which holds last epoch; for exact best, reload)\n",
    "    try:\n",
    "        best_model = AutoModelForSequenceClassification.from_pretrained(out_dir).to(DEVICE)  # Reload best if present\n",
    "    except Exception:\n",
    "        best_model = model                                  # Otherwise use last-epoch model\n",
    "    test_acc, test_f1 = epoch_eval(best_model, test_loader, DEVICE)  # Final test evaluation\n",
    "\n",
    "    # Save summary row\n",
    "    row = {\n",
    "        \"model\": model_name, \"hf_id\": model_id, \"regime_frac\": regime_frac,  # Identity fields\n",
    "        \"train_size\": len(ds_tr), \"dev_size\": len(ds_dv), \"test_size\": len(ds_ts),  # Dataset sizes\n",
    "        \"best_dev_f1\": float(best_dev_f1), \"test_acc\": float(test_acc), \"test_f1\": float(test_f1)  # Metrics\n",
    "    }\n",
    "    pd.DataFrame(history).to_csv(os.path.join(out_dir, \"train_history.csv\"), index=False, encoding=\"utf-8-sig\")  # Persist per-epoch log\n",
    "    pd.DataFrame([row]).to_csv(os.path.join(out_dir, \"summary.csv\"), index=False, encoding=\"utf-8-sig\")          # Persist run summary\n",
    "    return row                                               # Return summary dict to caller\n",
    "\n",
    "\n",
    "# MAIN \n",
    "\n",
    "# Function: main\n",
    "# Purpose: End-to-end orchestration—load splits, prepare DataFrames, and iterate model × regime runs; collect summaries.\n",
    "# Inputs: \n",
    "#   None (uses module-level config paths and constants).\n",
    "# Outputs: \n",
    "#   None (Files written under OUT_DIR; prints run tables).\n",
    "def main():\n",
    "    random.seed(SEED); np.random.seed(SEED); set_seed(SEED)  # Seed Python, NumPy, and Transformers\n",
    "\n",
    "    train = load_json_array(os.path.join(DATA_DIR, \"wic_train.json\"))  # Read train split JSON\n",
    "    dev   = load_json_array(os.path.join(DATA_DIR, \"wic_dev.json\"))    # Read dev split JSON\n",
    "    test  = load_json_array(os.path.join(DATA_DIR, \"wic_test.json\"))   # Read test split JSON\n",
    "\n",
    "    df_train = to_dataframe(train)                          # Convert train JSON → DataFrame\n",
    "    df_dev   = to_dataframe(dev)                            # Convert dev JSON → DataFrame\n",
    "    df_test  = to_dataframe(test)                           # Convert test JSON → DataFrame\n",
    "\n",
    "    runs_dir = os.path.join(OUT_DIR, \"runs\"); os.makedirs(runs_dir, exist_ok=True)  # Ensure runs folder exists\n",
    "    results = []                                            # Accumulate per-run summaries\n",
    "    for model_name, model_id in MODELS.items():             # Iterate models\n",
    "        for frac in REGIMES:                                # Iterate data regimes\n",
    "            try:\n",
    "                res = train_eval_one(model_id, model_name, df_train, df_dev, df_test, frac, runs_dir)  # Run a job\n",
    "                results.append(res)                         # Keep summary row\n",
    "            except Exception as e:                          # Robustness: catch & log errors per run\n",
    "                print(f\"[WARN] Run failed for {model_name} (frac={frac}): {e}\")\n",
    "\n",
    "    if results:                                             # If at least one successful run\n",
    "        df_res = pd.DataFrame(results)                      # Build a table of summaries\n",
    "        df_res.to_csv(os.path.join(OUT_DIR, \"ALL_results.csv\"), index=False, encoding=\"utf-8-sig\")  # Save summaries CSV\n",
    "        piv = df_res.pivot_table(index=[\"model\",\"regime_frac\"], values=[\"test_acc\",\"test_f1\"], aggfunc=\"mean\")  # Pivot by model/regime\n",
    "        piv = piv.reset_index().sort_values([\"model\",\"regime_frac\"])  # Sort for readability\n",
    "        piv.to_csv(os.path.join(OUT_DIR, \"ALL_results_pivot.csv\"), index=False, encoding=\"utf-8-sig\")  # Save pivot CSV\n",
    "        print(\"\\n== Final (test) results ==\\n\", piv)        # Print final pivot table\n",
    "    else:\n",
    "        print(\"No successful runs to summarize.\")           # Inform if nothing completed\n",
    "\n",
    "if __name__ == \"__main__\":                                  # Script entry point\n",
    "    main()                                                  # Launch the pipeline\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
